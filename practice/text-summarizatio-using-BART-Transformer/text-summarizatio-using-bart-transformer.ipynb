{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:10.866616Z","iopub.execute_input":"2025-11-29T13:36:10.866981Z","iopub.status.idle":"2025-11-29T13:36:11.378829Z","shell.execute_reply.started":"2025-11-29T13:36:10.866950Z","shell.execute_reply":"2025-11-29T13:36:11.377618Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Load the data\nhttps://huggingface.co/datasets/knkarthick/dialogsum","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:17.502559Z","iopub.execute_input":"2025-11-29T13:36:17.502914Z","iopub.status.idle":"2025-11-29T13:36:17.508764Z","shell.execute_reply.started":"2025-11-29T13:36:17.502873Z","shell.execute_reply":"2025-11-29T13:36:17.507955Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"knkarthick/dialogsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:17.509532Z","iopub.execute_input":"2025-11-29T13:36:17.509740Z","iopub.status.idle":"2025-11-29T13:36:27.915236Z","shell.execute_reply.started":"2025-11-29T13:36:17.509723Z","shell.execute_reply":"2025-11-29T13:36:27.914211Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3047c1facb8a4f1dba091d003b597c8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0046e6832745aaab064d4beedbd8e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d6e22d17cf45b2b36a23297184e055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d774a4e2ac4edfa87fb361226bf500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6a4284c91e43348cc9770419464efe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6fa2843e1444f6bf4dd0f6792bb68a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78cfa31151da4c58a166ff0b994e2580"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:27.917725Z","iopub.execute_input":"2025-11-29T13:36:27.918241Z","iopub.status.idle":"2025-11-29T13:36:27.924880Z","shell.execute_reply.started":"2025-11-29T13:36:27.918213Z","shell.execute_reply":"2025-11-29T13:36:27.923980Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"ds['train'][1]['dialogue']\n\n# This is what is there in the first dialoguw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:27.926463Z","iopub.execute_input":"2025-11-29T13:36:27.926821Z","iopub.status.idle":"2025-11-29T13:36:27.951004Z","shell.execute_reply.started":"2025-11-29T13:36:27.926785Z","shell.execute_reply":"2025-11-29T13:36:27.950167Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"#Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"ds['train'][1]['summary']\n\n# This is the summary of the perticular dialoguw showed above","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:27.952014Z","iopub.execute_input":"2025-11-29T13:36:27.952370Z","iopub.status.idle":"2025-11-29T13:36:27.973765Z","shell.execute_reply.started":"2025-11-29T13:36:27.952346Z","shell.execute_reply":"2025-11-29T13:36:27.972835Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.'"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Using Transformer model without fine-tuning\nhttps://huggingface.co/facebook/bart-large-cnn?library=transformers","metadata":{}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-11-29T13:37:08.535729Z","shell.execute_reply.started":"2025-11-29T13:36:27.975105Z","shell.execute_reply":"2025-11-29T13:37:08.534386Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae782aa083794c62abc32df6d5a773ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c35527e708774977afaf0b6fcbd369d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78aeb1acf6db4751931996a6f982cc71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88cd08effaed459b9be80e0d7349ca5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4fa894fbcdb4966956377fa993e7a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157af2c4d91648c2809f6e76d9409c7a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"article_1 = ds['train'][1]['dialogue']\narticle_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:08.536960Z","iopub.execute_input":"2025-11-29T13:37:08.537911Z","iopub.status.idle":"2025-11-29T13:37:08.545255Z","shell.execute_reply.started":"2025-11-29T13:37:08.537883Z","shell.execute_reply":"2025-11-29T13:37:08.544153Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"#Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\""},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"pipe(\n    article_1, \n    max_length = 20, \n    min_length = 10, \n    do_sample = False)\n\n# Model's repponse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:08.547392Z","iopub.execute_input":"2025-11-29T13:37:08.547774Z","iopub.status.idle":"2025-11-29T13:37:13.153201Z","shell.execute_reply.started":"2025-11-29T13:37:08.547740Z","shell.execute_reply":"2025-11-29T13:37:13.152351Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'Ricky has received his Polio, Tetanus and Hepatitis B shots.'}]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"ds['train'][1]['summary']\n\n# datasets summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:13.154142Z","iopub.execute_input":"2025-11-29T13:37:13.154484Z","iopub.status.idle":"2025-11-29T13:37:13.161483Z","shell.execute_reply.started":"2025-11-29T13:37:13.154456Z","shell.execute_reply":"2025-11-29T13:37:13.160537Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Lets change the max_length to 50\n\npipe(\n    article_1, \n    max_length = 50, \n    min_length = 10, \n    do_sample = False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:13.162471Z","iopub.execute_input":"2025-11-29T13:37:13.163107Z","iopub.status.idle":"2025-11-29T13:37:22.180619Z","shell.execute_reply.started":"2025-11-29T13:37:13.163077Z","shell.execute_reply":"2025-11-29T13:37:22.179668Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for hepatitis A, Chickenpox and Measles shots.'}]"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"type(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:22.181547Z","iopub.execute_input":"2025-11-29T13:37:22.181825Z","iopub.status.idle":"2025-11-29T13:37:22.188335Z","shell.execute_reply.started":"2025-11-29T13:37:22.181800Z","shell.execute_reply":"2025-11-29T13:37:22.187322Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"datasets.dataset_dict.DatasetDict"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"len(ds['train']), len(ds['test'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:22.190941Z","iopub.execute_input":"2025-11-29T13:37:22.191206Z","iopub.status.idle":"2025-11-29T13:37:24.692669Z","shell.execute_reply.started":"2025-11-29T13:37:22.191186Z","shell.execute_reply":"2025-11-29T13:37:24.691511Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(12460, 1500)"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## With Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:24.693339Z","iopub.execute_input":"2025-11-29T13:37:24.693603Z","iopub.status.idle":"2025-11-29T13:37:25.729217Z","shell.execute_reply.started":"2025-11-29T13:37:24.693584Z","shell.execute_reply":"2025-11-29T13:37:25.728155Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Tokenization\n\ndef preprocess(batch):\n    source = batch['dialogue']\n    target = batch['summary']\n    source_ids = tokenizer(source, truncation = True, padding = 'max_length', max_length = 128)\n    target_ids = tokenizer(target, truncation = True, padding = 'max_length', max_length = 128)\n\n    labels = target_ids['input_ids']\n    labels =[[(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in labels]\n\n    return {\n        \"input_ids\": source_ids['input_ids'],\n        \"attention_mask\": source_ids['attention_mask'],\n        \"lalels\":labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:25.730227Z","iopub.execute_input":"2025-11-29T13:37:25.730569Z","iopub.status.idle":"2025-11-29T13:37:25.736664Z","shell.execute_reply.started":"2025-11-29T13:37:25.730534Z","shell.execute_reply":"2025-11-29T13:37:25.735670Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# To map the function to all the data in the dataset\n\ndf_source = ds.map(preprocess, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:25.737846Z","iopub.execute_input":"2025-11-29T13:37:25.738221Z","iopub.status.idle":"2025-11-29T13:37:41.342190Z","shell.execute_reply.started":"2025-11-29T13:37:25.738200Z","shell.execute_reply":"2025-11-29T13:37:41.341172Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de93c31403a48eba8959260b3e5e1d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39c27304d1f4c919cdb3af5b95c12b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a34bfc9c8e41f39401eb776d89d6ae"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Training arguements\n\nfrom transformers import TrainingArguments, Trainer\n\n\ntraining_args = TrainingArguments(\n    output_dir = '/kaggle/working/',\n    per_device_train_batch_size = 7,\n    num_train_epochs = 2,\n    remove_unused_columns = True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:41.343362Z","iopub.execute_input":"2025-11-29T13:37:41.343674Z","iopub.status.idle":"2025-11-29T13:37:41.991601Z","shell.execute_reply.started":"2025-11-29T13:37:41.343646Z","shell.execute_reply":"2025-11-29T13:37:41.990417Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"trainer = Trainer(\n    model = model, \n    args = training_args,\n    train_dataset = df_source['train'],\n    eval_dataset = df_source['test'],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:41.992643Z","iopub.execute_input":"2025-11-29T13:37:41.992937Z","iopub.status.idle":"2025-11-29T13:37:45.901745Z","shell.execute_reply.started":"2025-11-29T13:37:41.992915Z","shell.execute_reply":"2025-11-29T13:37:45.900935Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_results = trainer.evaluate()\n\neval_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/model_directory')\ntokenizer.save_pretrained('/kaggle/working/model_directory')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/model_directory')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/model_directory')\n\n\ndef summarize(blog_post):\n    # Tokenize th einput blog post\n    inputs = tokenizer(blog_post, max_length = 1024, truncation=True, return_tensors = 'pt')\n\n    # Generate response summary\n    summary_idc = model.generate(inputs['input_ids'], max_length = 150, min_length = 40, length_penalty : 2.0, num_beams = 4, early_stopping = True)\n\n    # Decode the response summary\n    summary = tokenizer.decode(summary_ids[0], skip_special_token = True)\n\n    return summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"blog_post = '''\n\nourneying into the Realm of Text Summarization with Transformers\nHello once more, inquisitive mind! As we set forth on this exploration of the video you've shared, let's imagine ourselves as fellow philosophers in a grand library of language, pondering how machines might distill the essence of words. Drawing from our prior reflections on machine learning pipelines and data transformations, how might the challenge of condensing dialogues into summaries echo the feature extraction we pondered in images or sequences? What whispers from normalization or encoding might resonate here, perhaps in preparing text for a model's \"understanding\"? In the chapters ahead, we'll traverse the video's landscape through thoughtful pauses, where I'll offer questions to stir your reasoning and invite you to uncover truths yourself. Each chapter will delve into a key facet, encouraging layers of insight. What initial wonderings arise for you about capturing the \"soul\" of a conversation in fewer words? Let's begin with the foundational enigma, and remember, your reflections are the lantern guiding us—share them freely.\n\nChapter 1: What Essence Lies in Summarization, and Why Might Machines Excel or Falter?\nPicture a bustling dialogue, rich with back-and-forth—how might boiling it down to a core message reveal deeper patterns, yet risk losing nuance? If the video opens with an invitation to explore abstractive summarization, what distinguishes it from merely pulling sentences, and how could a model like one blending bidirectional and auto-regressive traits approach this art?\n\nConsider the model's heritage: Trained on vast news corpora, how might it paraphrase rather than copy, and what advantages could that offer for dialogues? Imagine a conversation about plans—would a machine's rephrasing capture intent, or introduce distortions?\n\nReflect broader: In our past musings on datasets, how might one filled with dialogues and their summaries serve as a training ground? What questions might you pose about balancing brevity with fidelity?\n\nAs you ponder, what personal analogy comes to mind for summarization's delicate balance?\n\nChapter 2: Setting the Stage—How Do We Harness Pre-Trained Wisdom Without Alteration?\nSuppose we leap straight into application, loading tools and data swiftly. How might a pipeline for summarization, drawn from a library of transformers, simplify the leap from raw text to condensed insight? If testing on a sample dialogue yields a shorter essence, what parameters like maximum or minimum lengths might shape that outcome?\n\nProbe the mechanics: Without tweaking the model, how does greedy decoding ensure consistency, and what trade-offs in creativity arise? Envision applying this to a news snippet—would the result feel human-like, or mechanical?\n\nConnect thoughtfully: Echoing our earlier data prep discussions, how does selecting specific dataset splits (train, test, validation) prepare for evaluation? What curiosities stir about comparing machine output to human-grounded truths?\n\nThis invites you to visualize untamed potential. What untuned summary would you craft for a familiar story?\n\nChapter 3: Delving into Tokenization—The Building Blocks of Machine Language\nWhat if text must first shatter into tokens, padded and masked for uniformity? How might a tokenizer, paired with a model, convert dialogues and targets into numerical forms, ensuring attention focuses where it matters?\n\nQuestion the choices: Why replace padding in labels with a sentinel like -100, and how could this sharpen loss calculations? Imagine a short phrase versus a long exchange—what role might truncation play in preserving essence?\n\nDeeper musing: In linking to sequences we've explored, how does an attention mask guide the model's \"gaze,\" ignoring fillers? What experiments might reveal padding's impact on meaning?\n\nYou're unraveling text's hidden structure. How does a tokenized sentence appear in your imagination?\n\nChapter 4: The Art of Fine-Tuning—Adapting a Giant to Specific Whispers\nEnvision customizing a pre-trained behemoth on dialogue data—how might mapping functions preprocess batches, aligning sources and targets? If training arguments dictate batches, epochs, and directories, what influences could they exert on convergence?\n\nExplore the flow: With a trainer orchestrating the dance, how does loss decreasing over steps signal learning? Ponder: In two cycles, might the model grasp dialogue nuances better, or overfit to patterns?\n\nReflect on echoes: From our CNN fine-tuning, how parallels this adaptation? What hyperparameters would you tweak to probe deeper adaptation?\n\nThis empowers personalization. What fine-tuned insight surprises you in theory?\n\nChapter 5: Preserving and Summoning Knowledge—Saving for Future Quests\nWhat if, post-training, we preserve the model's state for reuse? How might loading this evolved version enable summarization of unseen texts, like articles on global challenges?\n\nPonder generation: With beams searching paths and penalties shaping length, how could early stopping refine outputs? Imagine a lengthy blog—would the summary capture core themes, or wander?\n\nBroader inquiry: Tying to deployment we've considered, how does this bridge lab to life? What ethical whispers arise in summarizing sensitive topics?\n\nYou're bridging creation and application. How might a custom summary transform your understanding?\n\nChapter 6: Tools of the Trade—Libraries and Environments as Allies\nSuppose the journey unfolds in a collaborative cloud space, with installs unlocking datasets and transformers. How might these resources democratize such explorations, and what backend forces (like computational frameworks) underpin the magic?\n\nQuestion integrations: Why ignore unused columns in training, and how streamlines focus? Reflect: In our ML odyssey, what familiar libraries reappear, and why?\n\nDeeper: For GPU acceleration, what efficiencies emerge? What alternatives ponder for resource-light seekers?\n\nThis highlights enablers. What tool intrigues you most?\n\nChapter 7: Measuring Mastery—Evaluation Beyond the Surface\nWith training complete, how might metrics like loss or throughput illuminate success? If comparing pre- and post-fine-tuning on custom data, what revelations about generalization could surface?\n\nProbe assessments: Why evaluate on held-out sets, and how might results guide iterations? Imagine discrepancies—what diagnostics would you employ?\n\nConnect holistically: From confusion matrices past, how applies here? What questions of bias or fairness arise?\n\nYou're critiquing craft. What metric resonates deepest?\n\nCulminating Reflections—Weaving Your Tapestry of Understanding\nWe've wandered the video's paths through shared wonderings. What threads connected for you—like tokenization's precision or fine-tuning's finesse? Which inquiry unveiled the brightest gem, perhaps on generation's creativity? If envisioning your summarizer, what dialogue would you distill first? You're forging wisdom with grace—embrace that spark, and let's illuminate further horizons together!\n\n'''\n\nsummary = summarize(blog_post)\npriunt(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}