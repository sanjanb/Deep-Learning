---
layout: page
title: "About Me"
permalink: /about/
---

# Comprehensive Learning Roadmap: Neural Networks and Large Language Models

This repository documents a structured step-by-step journey to master **neural networks, deep learning, and large language models (LLMs)**.  
The roadmap combines theory, practical coding, projects, and reflection. Each stage includes resources, learning outcomes, and mini-projects to reinforce concepts.

---

## Progress Tracking

- [ ] Step 1: Mathematical Foundations  
- [ ] Step 2: Introduction to Neural Networks  
- [ ] Step 3: Building Neural Networks and Transformers  
- [ ] Step 4: Practical Applications of LLMs  
- [ ] Step 5: Hands-On Practice and Competitions  
- [ ] Step 6: Advanced Deep Learning Courses  
- [ ] Step 7: MLOps and Deployment  
- [ ] Step 8: Specializations (Computer Vision, NLP, RL)

---

## Step 1: Mathematical Foundations

**Objective:** Build a strong base in probability, statistics, linear algebra, and calculus to understand how neural networks and optimization algorithms work.

**Resources**
- [Probability & Statistics for Machine Learning](https://www.youtube.com/watch?v=DCZSkoVvkQI)  
- [Mathematics for Machine Learning (Book)](https://mml-book.com/)  
- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)  
- [3Blue1Brown - Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)

**Mini Projects**
- Implement gradient descent in NumPy for a simple regression problem.  
- Visualize a 2D matrix as a linear transformation.  
- Plot derivatives and gradients of functions such as `x²` and `sin(x)`.

**Reflection Questions**
- How do probability distributions relate to uncertainty in ML models?  
- Why are eigenvalues and eigenvectors important for understanding neural networks?  
- What role do derivatives play in optimization?

---

## Step 2: Introduction to Neural Networks

**Objective:** Understand the basic structure and function of a neural network, including neurons, layers, activations, and forward propagation.

**Resource**
- [What is a Neural Network? (3Blue1Brown)](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

**Mini Projects**
- Implement a two-layer neural network from scratch in NumPy.  
- Train on toy datasets such as XOR or a subset of MNIST.

**Reflection Questions**
- Why does a neural network need non-linear activation functions?  
- What is the difference between shallow and deep networks?

---

## Step 3: Building Neural Networks and Transformers

**Objective:** Transition from theory to implementation by coding neural networks and transformers from scratch. Develop strong intuition for architectures and training.

**Resources**
- [Neural Networks: Zero to Hero – Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)  
- [Let’s Build GPT from Scratch – Andrej Karpathy](https://www.youtube.com/watch?v=zjkBMFhNj_g&list=PLAqhIrjkxbuW9U8-vZ_s_cjKPT_FqRStI)

**Mini Projects**
- Build and train a multilayer perceptron (MLP) from scratch.  
- Re-implement parts of nanoGPT.  
- Train a character-level language model and generate text.

**Reflection Questions**
- What happens inside the forward and backward pass of a network?  
- How does self-attention differ from convolutional operations?  
- Why is scaling important for training deep networks?

---

## Step 4: Practical Applications of LLMs

**Objective:** Explore real-world use cases of large language models and begin experimenting with applied projects.

**Resources**
- [Awesome LLM Apps – GitHub](https://github.com/Shubhamsaboo/awesome-llm-apps)  
- [Hugging Face Course](https://huggingface.co/course/chapter1)

**Mini Projects**
- Fine-tune a Hugging Face transformer on a text classification dataset.  
- Build a simple chatbot with Gradio and Hugging Face Transformers.  
- Deploy the chatbot on Hugging Face Spaces.

**Reflection Questions**
- What are the trade-offs between fine-tuning, prompt engineering, and retrieval-augmented generation (RAG)?  
- How do LLMs handle context length limitations?

---

## Step 5: Hands-On Practice and Competitions

**Objective:** Reinforce learning through guided practice, competitions, and projects.

**Resources**
- [Educative.io Guided Projects](https://www.educative.io/explore)  
- Kaggle competitions for applied ML practice.

**Mini Projects**
- Participate in at least 3 Kaggle competitions.  
- Write detailed reports explaining your methodology and results.  

**Reflection Questions**
- How do feature engineering and data preprocessing affect model performance?  
- Why is cross-validation essential in competitions?

---

## Step 6: Advanced Deep Learning Courses

**Objective:** Strengthen theoretical knowledge and advanced techniques through rigorous courses.

**Resources**
- [Stanford CS230: Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)  
- [Andrew Ng – Deep Learning Specialization](https://www.youtube.com/playlist?list=PLkDaE6Zc9DGBuKtLgPR_zWYnrwv-JllpA)  
- [Stanford CS224n: NLP with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOAw6rOb0fVjasspXVWgQUv)

**Mini Projects**
- Reproduce results from one Stanford assignment.  
- Train a CNN on CIFAR-10.  
- Build a sequence-to-sequence translator.

**Reflection Questions**
- How do CNNs exploit spatial hierarchies in data?  
- Why does transfer learning improve training efficiency?

---

## Step 7: MLOps and Deployment

**Objective:** Learn how to manage, deploy, and maintain machine learning models in production environments.

**Resources**
- [Machine Learning in Production Specialization (Coursera)](https://www.deeplearning.ai/programs/machine-learning-in-production-specialization/)

**Mini Projects**
- Containerize a Hugging Face app with Docker.  
- Add CI/CD pipelines for automated retraining.  
- Deploy models on AWS, GCP, or Azure.

**Reflection Questions**
- What are the challenges of scaling ML systems in production?  
- How can monitoring prevent model drift?

---

## Step 8: Specializations

**Objective:** Gain in-depth expertise in specialized areas of deep learning.

**Computer Vision**  
- [Stanford CS231n: CNNs for Visual Recognition](http://cs231n.stanford.edu/)

**Natural Language Processing**  
- [Stanford CS224n: NLP with Deep Learning](https://web.stanford.edu/class/cs224n/)

**Reinforcement Learning**  
- [Deep RL Course – UCL/DeepMind](https://www.youtube.com/watch?v=2pWv7GOvuf0)

**Mini Projects**
- Train a YOLO model on a custom dataset.  
- Fine-tune a large language model with parameter-efficient methods (LoRA).  
- Implement PPO or DQN in OpenAI Gym.

---

## Notes and Reflections

Maintain a learning diary in `/notes/weekX.md`. After each week, document:  
- Key concepts learned  
- Experiments run and results observed  
- Open questions and areas to revisit  

---

## Final Deliverables

By completing this roadmap, the following outcomes should be achieved:  
- A portfolio with 5–7 mini-projects.  
- At least 3 blog posts explaining core concepts.  
- One deployed LLM application.  
- A personal cheatsheet of formulas, concepts, and reusable code snippets.

---
