{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:42:49.670049Z","iopub.execute_input":"2025-11-26T02:42:49.670743Z","iopub.status.idle":"2025-11-26T02:42:50.136790Z","shell.execute_reply.started":"2025-11-26T02:42:49.670719Z","shell.execute_reply":"2025-11-26T02:42:50.136040Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:44:59.516052Z","iopub.execute_input":"2025-11-26T02:44:59.516440Z","iopub.status.idle":"2025-11-26T02:45:06.108874Z","shell.execute_reply.started":"2025-11-26T02:44:59.516413Z","shell.execute_reply":"2025-11-26T02:45:06.108005Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Build the CNN model","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, in_channels = 1, num_classes = 10):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3, 3), stride = (1,1), padding = (1, 1))\n        self.pool = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))\n        \n        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3), stride = (1,1), padding = (1, 1))\n        self.fc1 = nn.Linear(16*7*7, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc1(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:49:04.654712Z","iopub.execute_input":"2025-11-26T02:49:04.655032Z","iopub.status.idle":"2025-11-26T02:49:04.663107Z","shell.execute_reply.started":"2025-11-26T02:49:04.655007Z","shell.execute_reply":"2025-11-26T02:49:04.662062Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Set device","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:48:05.235090Z","iopub.execute_input":"2025-11-26T02:48:05.235438Z","iopub.status.idle":"2025-11-26T02:48:05.240164Z","shell.execute_reply.started":"2025-11-26T02:48:05.235414Z","shell.execute_reply":"2025-11-26T02:48:05.239208Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Hyper parameters","metadata":{}},{"cell_type":"code","source":"in_channels = 1\nout_channels = 8\nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:48:05.541747Z","iopub.execute_input":"2025-11-26T02:48:05.542390Z","iopub.status.idle":"2025-11-26T02:48:05.546720Z","shell.execute_reply.started":"2025-11-26T02:48:05.542364Z","shell.execute_reply":"2025-11-26T02:48:05.545948Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"transform = transforms.ToTensor() \n\n# 1. Training Dataset\ntrain_dataset = datasets.MNIST(\n    root='./data', \n    train=True, \n    transform=transform, \n    download=True\n)\n\n# 2. Test Dataset\ntest_dataset = datasets.MNIST(\n    root='./data', \n    train=False, \n    transform=transform, \n    download=True \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:45:21.356649Z","iopub.execute_input":"2025-11-26T02:45:21.356961Z","iopub.status.idle":"2025-11-26T02:45:26.490327Z","shell.execute_reply.started":"2025-11-26T02:45:21.356938Z","shell.execute_reply":"2025-11-26T02:45:26.489398Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 12.3MB/s]\n100%|██████████| 28.9k/28.9k [00:00<00:00, 367kB/s]\n100%|██████████| 1.65M/1.65M [00:00<00:00, 3.40MB/s]\n100%|██████████| 4.54k/4.54k [00:00<00:00, 6.26MB/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\ntest_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:45:26.491548Z","iopub.execute_input":"2025-11-26T02:45:26.491837Z","iopub.status.idle":"2025-11-26T02:45:26.496924Z","shell.execute_reply.started":"2025-11-26T02:45:26.491814Z","shell.execute_reply":"2025-11-26T02:45:26.496008Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Initialize the network","metadata":{}},{"cell_type":"code","source":"model = CNN().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:49:07.936220Z","iopub.execute_input":"2025-11-26T02:49:07.936817Z","iopub.status.idle":"2025-11-26T02:49:07.943936Z","shell.execute_reply.started":"2025-11-26T02:49:07.936792Z","shell.execute_reply":"2025-11-26T02:49:07.943188Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:49:11.725922Z","iopub.execute_input":"2025-11-26T02:49:11.726512Z","iopub.status.idle":"2025-11-26T02:49:11.731015Z","shell.execute_reply.started":"2025-11-26T02:49:11.726486Z","shell.execute_reply":"2025-11-26T02:49:11.730125Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Train the network","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Get data to cuda if possible\n        data = data.to(device = device)\n        targets = targets.to(device = device)\n\n        # Farward pass\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Gradient Descent\n        optimizer.step() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:49:13.475959Z","iopub.execute_input":"2025-11-26T02:49:13.476736Z","iopub.status.idle":"2025-11-26T02:49:58.023138Z","shell.execute_reply.started":"2025-11-26T02:49:13.476705Z","shell.execute_reply":"2025-11-26T02:49:58.021944Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Check accuracy on tarining and test, to look how good the model is performing","metadata":{}},{"cell_type":"code","source":"def check_accuracy(loader, model):\n    if loader.dataset.train:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Checking accuracy on testing data\")\n        \n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device = device)\n            y = y.to(device = device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:49:58.024556Z","iopub.execute_input":"2025-11-26T02:49:58.024839Z","iopub.status.idle":"2025-11-26T02:49:58.031555Z","shell.execute_reply.started":"2025-11-26T02:49:58.024818Z","shell.execute_reply":"2025-11-26T02:49:58.030193Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model.train()\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T02:49:58.032581Z","iopub.execute_input":"2025-11-26T02:49:58.033315Z","iopub.status.idle":"2025-11-26T02:50:10.633337Z","shell.execute_reply.started":"2025-11-26T02:49:58.033267Z","shell.execute_reply":"2025-11-26T02:50:10.632356Z"}},"outputs":[{"name":"stdout","text":"Checking accuracy on training data\nGot 58888 / 60000 with accuracy 98.15\nChecking accuracy on testing data\nGot 9790 / 10000 with accuracy 97.90\n","output_type":"stream"}],"execution_count":25}]}