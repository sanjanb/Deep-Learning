{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:51:04.218384Z","iopub.execute_input":"2025-10-11T01:51:04.218814Z","iopub.status.idle":"2025-10-11T01:51:04.225762Z","shell.execute_reply.started":"2025-10-11T01:51:04.218775Z","shell.execute_reply":"2025-10-11T01:51:04.224699Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# **Image Classification**\n\n## 1. Introduction\n\n**Image Classification** is a fundamental task in computer vision where the goal is to assign a label (or category) to an input image.\nFor example:\n\n* Classifying an image as “cat” or “dog”.\n* Detecting the presence of diseases in medical scans.\n\nFormally, given an image ( x ), the model predicts the class label ( y \\in {1, 2, ..., K} ), where ( K ) is the number of categories.\n\n\n## 2. Core Idea\n\nAn image classification model learns to map pixel patterns to class labels:\n[\nf_\\theta(x) \\rightarrow y\n]\nwhere ( f_\\theta ) is a neural network parameterized by weights ( \\theta ).\n\nThe model minimizes a **loss function** (e.g., cross-entropy) to make predictions as close as possible to the true labels.\n\n\n## 3. Workflow Overview\n\n1. **Data Preparation**\n\n   * Collect a labeled dataset (e.g., CIFAR-10, MNIST, ImageNet).\n   * Split into training, validation, and test sets.\n   * Apply preprocessing (resize, normalization, augmentation).\n\n2. **Model Design**\n\n   * Choose a neural network architecture (e.g., CNN, ResNet, EfficientNet).\n   * Define input shape (e.g., 224×224×3 for RGB images).\n\n3. **Training**\n\n   * Feed batches of images to the network.\n   * Compute the **loss** between predictions and true labels.\n   * Update weights via **backpropagation** using an optimizer (SGD, Adam).\n\n4. **Evaluation**\n\n   * Use accuracy, precision, recall, and F1-score on test data.\n\n5. **Deployment**\n\n   * Save the trained model and use it for inference on new images.\n\n\n## 4. Common Architectures\n\n| Model            | Key Features                                     | Notable Use                         |\n| ---------------- | ------------------------------------------------ | ----------------------------------- |\n| **LeNet-5**      | Early CNN; simple architecture for MNIST         | Digit recognition                   |\n| **AlexNet**      | Deep CNN with ReLU and Dropout                   | ImageNet 2012                       |\n| **VGGNet**       | Uniform 3×3 conv layers                          | Transfer learning                   |\n| **ResNet**       | Residual connections to solve vanishing gradient | General purpose                     |\n| **EfficientNet** | Compound scaling of depth, width, resolution     | High accuracy with fewer parameters |\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:51:04.227313Z","iopub.execute_input":"2025-10-11T01:51:04.227695Z","iopub.status.idle":"2025-10-11T01:51:04.248754Z","shell.execute_reply.started":"2025-10-11T01:51:04.227659Z","shell.execute_reply":"2025-10-11T01:51:04.247746Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## 5. Example: Basic CNN in Keras\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Load dataset (CIFAR-10)\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Define model\nmodel = models.Sequential([\n    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile and train\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:51:04.249832Z","iopub.execute_input":"2025-10-11T01:51:04.250200Z","iopub.status.idle":"2025-10-11T01:51:07.964163Z","shell.execute_reply.started":"2025-10-11T01:51:04.250149Z","shell.execute_reply":"2025-10-11T01:51:07.963239Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:51:07.965991Z","iopub.execute_input":"2025-10-11T01:51:07.966452Z","iopub.status.idle":"2025-10-11T01:51:07.986142Z","shell.execute_reply.started":"2025-10-11T01:51:07.966420Z","shell.execute_reply":"2025-10-11T01:51:07.984876Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:51:07.987314Z","iopub.execute_input":"2025-10-11T01:51:07.988009Z","iopub.status.idle":"2025-10-11T01:57:06.337221Z","shell.execute_reply.started":"2025-10-11T01:51:07.987974Z","shell.execute_reply":"2025-10-11T01:57:06.336028Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - accuracy: 0.3440 - loss: 1.7669 - val_accuracy: 0.5330 - val_loss: 1.2978\nEpoch 2/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 23ms/step - accuracy: 0.5765 - loss: 1.1915 - val_accuracy: 0.6250 - val_loss: 1.0535\nEpoch 3/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 23ms/step - accuracy: 0.6374 - loss: 1.0255 - val_accuracy: 0.6444 - val_loss: 1.0270\nEpoch 4/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 23ms/step - accuracy: 0.6842 - loss: 0.9052 - val_accuracy: 0.6807 - val_loss: 0.9001\nEpoch 5/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 22ms/step - accuracy: 0.7092 - loss: 0.8303 - val_accuracy: 0.6868 - val_loss: 0.8916\nEpoch 6/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 22ms/step - accuracy: 0.7318 - loss: 0.7718 - val_accuracy: 0.6865 - val_loss: 0.8982\nEpoch 7/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 22ms/step - accuracy: 0.7486 - loss: 0.7234 - val_accuracy: 0.6777 - val_loss: 0.9225\nEpoch 8/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 22ms/step - accuracy: 0.7610 - loss: 0.6756 - val_accuracy: 0.6956 - val_loss: 0.8887\nEpoch 9/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 22ms/step - accuracy: 0.7734 - loss: 0.6426 - val_accuracy: 0.7107 - val_loss: 0.8557\nEpoch 10/10\n\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 22ms/step - accuracy: 0.7877 - loss: 0.6067 - val_accuracy: 0.6977 - val_loss: 0.9012\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a0d49efe710>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 6. Key Loss Function\n\nFor multi-class classification, we use **Categorical Cross-Entropy**:\n[\nL = -\\sum_{i=1}^{K} y_i \\log(\\hat{y_i})\n]\nwhere\n\n* ( y_i ) = true label (one-hot encoded)\n* ( \\hat{y_i} ) = predicted probability from softmax\n\n\n## 7. Performance Metrics\n\n* **Accuracy**: ( \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} )\n* **Precision & Recall**: Useful for imbalanced datasets.\n* **Confusion Matrix**: Visualizes prediction vs true class.\n\n\n## 8. Techniques to Improve Accuracy\n\n* **Data Augmentation**: Flip, rotate, crop, color jitter.\n* **Transfer Learning**: Use pre-trained models (e.g., ResNet, MobileNet).\n* **Regularization**: Dropout, L2 weight decay.\n* **Batch Normalization**: Stabilizes and speeds up training.\n* **Learning Rate Scheduling**: Gradually reduce learning rate.\n\n## 9. Real-World Applications\n\n* Facial recognition\n* Object detection\n* Medical imaging\n* Quality inspection in manufacturing\n* Satellite image analysis\n","metadata":{}},{"cell_type":"markdown","source":"# **Image Classification with Localization**\n\n## 1. Introduction\n\n**Image Classification with Localization** extends simple classification by not only identifying *what* is in an image but also *where* it is.\nThe goal is to predict both:\n\n1. The **class label** of the object.\n2. The **bounding box coordinates** that locate the object in the image.\n\nFormally, given an input image ( x ), the model predicts:\n[\n\\hat{y} = [p_1, p_2, ..., p_K, b_x, b_y, b_w, b_h]\n]\nwhere:\n\n* ( p_1, p_2, ..., p_K ) are class probabilities (via softmax).\n* ( b_x, b_y, b_w, b_h ) define the bounding box (center coordinates, width, height).\n\n\n## 2. Difference from Pure Classification\n\n| Task                                 | Output               | Example Output                      |\n| ------------------------------------ | -------------------- | ----------------------------------- |\n| **Image Classification**             | Label only           | “Dog”                               |\n| **Classification with Localization** | Label + Bounding Box | (“Dog”, [x=75, y=40, w=120, h=150]) |\n\nThis means the model learns both *semantic* (what) and *spatial* (where) information.\n\n\n## 3. Problem Setup\n\n1. **Input:** Image (e.g., 224×224×3)\n2. **Output:**\n\n   * **Class scores:** vector of length (K) (for K classes)\n   * **Bounding box:** vector of 4 numbers ([x, y, w, h])\n3. **Model Type:** CNN backbone with multiple output heads.\n\n\n## 4. Model Architecture\n\nA typical model has:\n\n* A **feature extractor** (like ResNet, VGG).\n* Two **output heads**:\n\n  1. **Classification head:** predicts object class.\n  2. **Regression head:** predicts bounding box coordinates.\n\nExample structure:\n\n```\nInput Image\n     ↓\nConvolutional Layers (feature extraction)\n     ↓\nFlatten / Global Pooling\n     ↓\n ┌──────────────┬──────────────┐\n │ Classification │ Localization │\n │ (Softmax)      │ (Linear)     │\n └──────────────┴──────────────┘\n```\n\n\n## 5. Combined Loss Function\n\nThe model optimizes **both classification and localization losses** together.\n\n[\nL = L_{cls} + \\lambda L_{loc}\n]\n\nWhere:\n\n* ( L_{cls} ) = categorical cross-entropy (for class prediction)\n* ( L_{loc} ) = mean squared error or smooth L1 loss (for bounding box regression)\n* ( \\lambda ) = balancing weight between the two losses\n\nExample:\n[\nL_{loc} = \\frac{1}{N} \\sum_i | b_i - \\hat{b_i} |^2\n]","metadata":{}},{"cell_type":"code","source":"\n## 6. Example Implementation (TensorFlow/Keras)\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, losses\n\n# Base feature extractor\nbase_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\nx = layers.GlobalAveragePooling2D()(base_model.output)\n\n# Classification head\nclass_output = layers.Dense(10, activation='softmax', name='class_output')(x)\n\n# Localization head\nbbox_output = layers.Dense(4, activation='linear', name='bbox_output')(x)\n\n# Combine into one model\nmodel = models.Model(inputs=base_model.input, outputs=[class_output, bbox_output])\n\n# Compile with multiple losses\nmodel.compile(\n    optimizer='adam',\n    loss={\n        'class_output': 'categorical_crossentropy',\n        'bbox_output': 'mse'\n    },\n    loss_weights={\n        'class_output': 1.0,\n        'bbox_output': 1.0\n    },\n    metrics={'class_output': 'accuracy'}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:57:06.338704Z","iopub.execute_input":"2025-10-11T01:57:06.338993Z","iopub.status.idle":"2025-10-11T01:57:08.715208Z","shell.execute_reply.started":"2025-10-11T01:57:06.338972Z","shell.execute_reply":"2025-10-11T01:57:08.714351Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nmodel.fit(X_train, {'class_output': y_classes, 'bbox_output': y_boxes}, epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 7. Bounding Box Formats\n\nThere are two common formats:\n\n1. **(x_min, y_min, x_max, y_max)** — coordinates of the box corners.\n2. **(x_center, y_center, width, height)** — center and size representation.\n\nAlways normalize coordinates to [0,1] relative to image dimensions during training.\n\n\n## 8. Dataset Examples\n\nDatasets that support localization (labels + bounding boxes):\n\n* **PASCAL VOC**\n* **MS COCO**\n* **Open Images Dataset**\n\nEach provides annotations in XML or JSON formats containing class names and box coordinates.\n\n\n## 9. Evaluation Metrics\n\n* **Classification:** Accuracy, Precision, Recall, F1-score\n* **Localization:** IoU (Intersection over Union)\n\n[\nIoU = \\frac{Area(Box_{pred} \\cap Box_{true})}{Area(Box_{pred} \\cup Box_{true})}\n]\n\nAn IoU > 0.5 is often considered a correct localization.\n---\n\n## 10. Extensions\n\nImage classification with localization forms the basis for more advanced tasks:\n\n* **Object Detection:** multiple objects per image (e.g., YOLO, SSD, Faster R-CNN)\n* **Instance Segmentation:** pixel-level object boundaries (e.g., Mask R-CNN)\n\n\n## 11. Summary\n\n| Aspect           | Description                                           |\n| ---------------- | ----------------------------------------------------- |\n| **Goal**         | Predict class + bounding box                          |\n| **Input**        | Image                                                 |\n| **Output**       | Class probabilities + 4 box coordinates               |\n| **Loss**         | Classification + Localization (MSE or Smooth L1)      |\n| **Metrics**      | Accuracy, IoU                                         |\n| **Applications** | Object tracking, medical imaging, autonomous vehicles |\n\n--","metadata":{}},{"cell_type":"markdown","source":"\n# **Object Detection**\n\n## 1. Introduction\n\n**Object Detection** is a computer vision task that identifies **what objects** are present in an image and **where** they are located.\nUnlike simple classification or single-object localization, object detection can handle **multiple objects** of different classes in the same image.\n\n**Goal:**\nFor a given image ( I ), predict a set of bounding boxes ( B_i ) and class labels ( C_i ) for all objects:\n[\n\\hat{y} = { (B_1, C_1), (B_2, C_2), ..., (B_n, C_n) }\n]\n\n\n## 2. Comparison with Related Tasks\n\n| Task                     | Output                           | Example                      |\n| ------------------------ | -------------------------------- | ---------------------------- |\n| **Image Classification** | One label per image              | “Cat”                        |\n| **Localization**         | One label + one bounding box     | (“Cat”, [x, y, w, h])        |\n| **Object Detection**     | Multiple labels + bounding boxes | (“Cat”, Box1), (“Dog”, Box2) |\n\n\n## 3. Core Concepts\n\nObject detection combines **classification** and **localization**:\n\n1. **Classification** — what the object is.\n2. **Bounding Box Regression** — where the object is.\n\nEach detected object is represented by:\n[\n[b_x, b_y, b_w, b_h, p_1, p_2, ..., p_K]\n]\nwhere:\n\n* ( b_x, b_y, b_w, b_h ): bounding box coordinates.\n* ( p_i ): class probabilities.\n\n\n## 4. Types of Object Detection Models\n\nObject detection models are typically categorized into two main families:\n\n### 4.1. **Two-Stage Detectors**\n\nThese methods first generate *region proposals* and then classify each one.\n\n| Model                   | Description                                                            |\n| ----------------------- | ---------------------------------------------------------------------- |\n| **R-CNN (2014)**        | Extracts region proposals, runs CNN on each region. Slow but accurate. |\n| **Fast R-CNN (2015)**   | Runs CNN once per image, uses RoI pooling for proposals.               |\n| **Faster R-CNN (2016)** | Adds a Region Proposal Network (RPN) for efficiency.                   |\n\nTwo-stage detectors are more accurate but computationally expensive.\n\n\n### 4.2. **Single-Stage Detectors**\n\nThese models predict bounding boxes and classes in a **single forward pass**.\n\n| Model                                   | Description                                                          |\n| --------------------------------------- | -------------------------------------------------------------------- |\n| **YOLO (You Only Look Once)**           | Divides image into grid cells and predicts boxes + classes directly. |\n| **SSD (Single Shot Multibox Detector)** | Predicts multiple boxes at different scales from feature maps.       |\n| **RetinaNet**                           | Introduces focal loss to handle class imbalance.                     |\n\nSingle-stage models are faster and suitable for real-time detection.\n\n\n## 5. Model Architecture (Example: YOLO)\n\n**YOLO Architecture Overview:**\n\n```\nInput Image (e.g., 416x416x3)\n      ↓\nConvolutional Backbone (e.g., Darknet53)\n      ↓\nFeature Maps\n      ↓\nDetection Head\n   ├── Bounding Box (x, y, w, h)\n   ├── Objectness Score\n   └── Class Probabilities\n```\n\nThe image is divided into an ( S \\times S ) grid.\nEach grid cell predicts:\n\n* ( B ) bounding boxes\n* Confidence score (objectness)\n* Class probabilities\n\n\n\n## 6. Loss Function\n\nThe overall loss combines **localization**, **confidence**, and **classification** losses.\n\n[\nL = \\lambda_{coord} L_{coord} + L_{conf} + L_{class}\n]\n\n* ( L_{coord} ): bounding box regression loss (Smooth L1 / MSE)\n* ( L_{conf} ): objectness loss\n* ( L_{class} ): categorical cross-entropy\n* ( \\lambda_{coord} ): weight for coordinate loss\n\n\n\n## 7. Evaluation Metrics\n\n### **Intersection over Union (IoU)**\n\nMeasures how much the predicted box overlaps with the ground truth.\n\n[\nIoU = \\frac{Area(B_{pred} \\cap B_{true})}{Area(B_{pred} \\cup B_{true})}\n]\n\nA prediction is considered correct if IoU > 0.5 (common threshold).\n\n\n\n### **Mean Average Precision (mAP)**\n\nThe standard metric for object detection:\n\n1. Compute precision-recall curve for each class.\n2. Compute **Average Precision (AP)** for each class.\n3. Take the mean across all classes → **mAP**.\n\n[\nmAP = \\frac{1}{K} \\sum_{i=1}^{K} AP_i\n]","metadata":{}},{"cell_type":"code","source":"\n## 8. Example Implementation (Using YOLOv8 and Ultralytics)\n\n# Install ultralytics if not already installed\n!pip install ultralytics\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLOv8 model\nmodel = YOLO('yolov8s.pt')\n\n# Perform object detection\nresults = model('image.jpg')\n\n# Visualize results\nresults.show()\n\n\n# You can fine-tune YOLO on a custom dataset by:\n\n\nmodel.train(data='data.yaml', epochs=50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 9. Popular Datasets\n\n| Dataset         | Description                              |\n| --------------- | ---------------------------------------- |\n| **COCO**        | 80 object categories, 330K images        |\n| **PASCAL VOC**  | 20 categories, classic benchmark         |\n| **Open Images** | 9M images with bounding boxes and labels |\n| **KITTI**       | Commonly used for autonomous driving     |\n\n\n\n## 10. Practical Applications\n\n* Autonomous Vehicles (detecting pedestrians, vehicles, signs)\n* Security Systems (person or object recognition)\n* Medical Imaging (detecting tumors or anomalies)\n* Retail Analytics (product detection on shelves)\n* Robotics (object tracking and manipulation)\n\n\n## 11. Summary\n\n| Aspect           | Description                                      |\n| ---------------- | ------------------------------------------------ |\n| **Goal**         | Detect and localize multiple objects in an image |\n| **Output**       | Multiple bounding boxes + class labels           |\n| **Models**       | Faster R-CNN, YOLO, SSD, RetinaNet               |\n| **Metrics**      | IoU, mAP                                         |\n| **Approach**     | Combines classification + localization           |\n| **Applications** | Real-time systems, autonomous tech, security     |\n\n\n## 12. Next Step\n\nOnce object detection is clear, the natural progression is:\n\n* **Instance Segmentation:** pixel-level detection (Mask R-CNN)\n* **Semantic Segmentation:** classifying every pixel in the image\n","metadata":{}},{"cell_type":"markdown","source":"# **Image Segmentation**\n\n## 1. Introduction\n\n**Image Segmentation** is the process of partitioning an image into multiple regions or segments to simplify its representation and make it more meaningful.\nThe goal is to assign a **label to every pixel** in the image so that pixels with the same label share similar characteristics (such as color, texture, or object identity).\n\nMathematically, for an image ( I ) of size ( H \\times W ), segmentation produces an output mask ( M ) where:\n[\nM_{ij} \\in {1, 2, ..., K}\n]\nHere, ( K ) is the number of classes, and each pixel ((i, j)) gets a class label.\n\n\n## 2. Why Segmentation?\n\nUnlike **object detection**, which predicts bounding boxes around objects, **segmentation** gives a **pixel-accurate** understanding of the image.\n\n| Task           | Output Type         | Example                                      |\n| -------------- | ------------------- | -------------------------------------------- |\n| Classification | One label per image | “Dog”                                        |\n| Detection      | Bounding boxes      | (“Dog”, Box1)                                |\n| Segmentation   | Pixel-level masks   | Every pixel labeled as dog, background, etc. |\n\n\n\n## 3. Types of Image Segmentation\n\n### 3.1. **Semantic Segmentation**\n\n* Groups pixels belonging to the same class.\n* All objects of the same type share one label.\n* Example: all cars → “car” class, regardless of instance.\n\n**Use case:** Satellite imagery, medical scans, scene understanding.\n\n\n### 3.2. **Instance Segmentation**\n\n* Distinguishes between **different instances** of the same class.\n* Example: two people → “person 1” and “person 2”.\n\n**Use case:** Autonomous driving, robotics, video analysis.\n\n\n### 3.3. **Panoptic Segmentation**\n\n* Combines both **semantic** and **instance** segmentation.\n* Produces a complete scene understanding.\n\n\n## 4. Architecture Overview\n\n### 4.1. **Encoder–Decoder Structure**\n\nMost segmentation networks follow an **encoder–decoder** architecture:\n\n* **Encoder:** extracts spatial and semantic features (similar to a CNN classifier).\n* **Decoder:** upsamples features to recover spatial resolution and produce a dense pixel map.\n\n```\nInput Image\n     ↓\nEncoder (e.g., ResNet, VGG)\n     ↓\nBottleneck (feature representation)\n     ↓\nDecoder (upsampling + skip connections)\n     ↓\nSegmentation Map (pixel labels)\n```\n\n\n\n## 5. Popular Segmentation Architectures\n\n| Model                                 | Description                                                                          |\n| ------------------------------------- | ------------------------------------------------------------------------------------ |\n| **FCN (Fully Convolutional Network)** | Replaces fully connected layers with convolutional layers for pixel-wise prediction. |\n| **U-Net**                             | Encoder–decoder with skip connections; popular in medical imaging.                   |\n| **SegNet**                            | Uses pooling indices for efficient upsampling.                                       |\n| **DeepLab (v3, v3+)**                 | Uses atrous (dilated) convolutions and CRFs for high accuracy.                       |\n| **Mask R-CNN**                        | Extends Faster R-CNN to perform instance segmentation.                               |\n\n\n\n## 6. Example: U-Net Architecture\n\nU-Net is one of the most widely used models for semantic segmentation.\n\n**Key features:**\n\n* Contracting path (encoder) captures context.\n* Expanding path (decoder) recovers spatial details.\n* Skip connections combine encoder and decoder features.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models\n\ndef unet_model(input_shape=(128,128,3), num_classes=2):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder\n    c1 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(inputs)\n    c1 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(c1)\n    p1 = layers.MaxPooling2D((2,2))(c1)\n\n    c2 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(p1)\n    c2 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(c2)\n    p2 = layers.MaxPooling2D((2,2))(c2)\n\n    # Bottleneck\n    b = layers.Conv2D(256, (3,3), activation='relu', padding='same')(p2)\n    b = layers.Conv2D(256, (3,3), activation='relu', padding='same')(b)\n\n    # Decoder\n    u1 = layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(b)\n    u1 = layers.concatenate([u1, c2])\n    c3 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(u1)\n    c3 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(c3)\n\n    u2 = layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c3)\n    u2 = layers.concatenate([u2, c1])\n    c4 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(u2)\n    c4 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(c4)\n\n    outputs = layers.Conv2D(num_classes, (1,1), activation='softmax')(c4)\n\n    model = models.Model(inputs, outputs)\n    return model\n\nmodel = unet_model()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T01:59:56.837295Z","iopub.execute_input":"2025-10-11T01:59:56.837664Z","iopub.status.idle":"2025-10-11T01:59:57.003257Z","shell.execute_reply.started":"2025-10-11T01:59:56.837638Z","shell.execute_reply":"2025-10-11T01:59:57.002389Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,792\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_4[\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │    \u001b[38;5;34m295,168\u001b[0m │ max_pooling2d_5[\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │    \u001b[38;5;34m590,080\u001b[0m │ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m131,200\u001b[0m │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m295,040\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m32,832\u001b[0m │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_transpose… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m73,792\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m130\u001b[0m │ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m2\u001b[0m)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │ max_pooling2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_transpose… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,862,914\u001b[0m (7.11 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,862,914</span> (7.11 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,862,914\u001b[0m (7.11 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,862,914</span> (7.11 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## 7. Loss Functions for Segmentation\n\nThe choice of loss depends on the nature of the segmentation problem.\n\n| Loss                   | Use Case                            | Formula / Description                              |\n| ---------------------- | ----------------------------------- | -------------------------------------------------- |\n| **Cross-Entropy Loss** | Multi-class segmentation            | Penalizes incorrect pixel classifications.         |\n| **Dice Loss**          | Medical imaging, imbalanced classes | Measures overlap between predicted and true masks. |\n| **IoU Loss (Jaccard)** | General segmentation                | Based on Intersection over Union metric.           |\n| **Focal Loss**         | Imbalanced datasets                 | Down-weights easy examples.                        |\n\nExample (Dice Coefficient):\n[\nDice = \\frac{2 |A \\cap B|}{|A| + |B|}\n]\n[\nLoss_{Dice} = 1 - Dice\n]\n\n\n## 8. Evaluation Metrics\n\n* **Pixel Accuracy**\n* **IoU (Intersection over Union)**\n* **Dice Coefficient (F1 Score)**\n* **Mean IoU (mIoU)** — average IoU over all classes.\n\n[\nmIoU = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{TP_i}{TP_i + FP_i + FN_i}\n]\n\n\n## 9. Example Inference Visualization\n\nAfter training, predictions can be visualized as overlays.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_segmentation(image, mask, pred_mask):\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(mask, cmap='gray')\n    plt.title(\"Ground Truth Mask\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(pred_mask, cmap='gray')\n    plt.title(\"Predicted Mask\")\n    plt.axis(\"off\")\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T02:00:01.835443Z","iopub.execute_input":"2025-10-11T02:00:01.835793Z","iopub.status.idle":"2025-10-11T02:00:01.842300Z","shell.execute_reply.started":"2025-10-11T02:00:01.835770Z","shell.execute_reply":"2025-10-11T02:00:01.841221Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## 10. Common Datasets\n\n| Dataset                 | Description                              |\n| ----------------------- | ---------------------------------------- |\n| **PASCAL VOC**          | 20 classes, benchmark dataset            |\n| **MS COCO**             | Complex real-world scenes                |\n| **Cityscapes**          | Urban street scenes (autonomous driving) |\n| **CamVid**              | Road scene understanding                 |\n| **ISIC / LUNA / DRIVE** | Medical segmentation datasets            |\n\n---\n\n## 11. Applications\n\n* **Medical Imaging:** Tumor or organ segmentation.\n* **Autonomous Driving:** Lane, vehicle, and pedestrian segmentation.\n* **Satellite Imagery:** Land cover classification.\n* **Agriculture:** Crop disease detection, field segmentation.\n* **AR/VR:** Background removal and object isolation.\n\n---\n\n## 12. Summary\n\n| Aspect            | Description                            |\n| ----------------- | -------------------------------------- |\n| **Goal**          | Assign a label to every pixel          |\n| **Input**         | Image                                  |\n| **Output**        | Pixel-level class mask                 |\n| **Architectures** | FCN, U-Net, SegNet, DeepLab            |\n| **Loss**          | Cross-Entropy, Dice, IoU               |\n| **Metrics**       | IoU, Dice, mIoU                        |\n| **Applications**  | Medical, automotive, satellite imaging |\n","metadata":{}}]}