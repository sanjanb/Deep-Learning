{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Sequence Modeling with Recurrent Neural Networks (RNN)**\n\n\n## 1. Introduction to Sequence Modeling\n\n**Sequence modeling** involves predicting or understanding data where the **order of elements matters**. Examples include:\n\n* Time series forecasting: stock prices, weather\n* Natural language processing: text, speech\n* Signal processing: audio, sensor readings\n* DNA sequence analysis: genomics\n\n**Challenge:** Traditional feedforward networks cannot capture **temporal dependencies**, because they process inputs independently.\n\n\n\n## 2. Recurrent Neural Networks (RNN)\n\nRNNs are designed for **sequence data**:\n\n* They have **hidden states** that are updated at each time step.\n* This allows RNNs to **remember information** from previous steps.\n* Formally:\n\n[\nh_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n]\n\n[\ny_t = g(W_{hy} h_t + b_y)\n]\n\nWhere:\n\n* (x_t) = input at time t\n* (h_t) = hidden state at time t\n* (y_t) = output at time t\n* (W_{xh}, W_{hh}, W_{hy}) = weight matrices\n* (f, g) = activation functions (e.g., tanh, softmax)\n\n**Key idea:** The hidden state (h_t) acts as a **memory** carrying information from previous inputs.\n\n\n\n## 3. Sequence Modeling Problem Example\n\n**Problem:** Predict the next word in a sentence (language modeling).\n\n**Input Sequence:**\n\n```\n\"The cat sat on the\"\n```\n\n**Target Output:**\n\n```\n\"mat\"\n```\n\n* Each word is encoded (one-hot or embedding).\n* RNN reads words **one at a time**, updates hidden state.\n* At the last step, the output predicts the next word.\n\n\n\n## 4. Steps to Solve Sequence Modeling with RNN\n\n### Step 1: Prepare the Data\n\n* Convert sequence to numerical representation (tokenization or embedding).\n* Split into input-output pairs:\n\n| Input Sequence | Target |\n| -------------- | ------ |\n| The            | cat    |\n| The cat        | sat    |\n| The cat sat    | on     |\n\n### Step 2: Build RNN Model\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n        out, _ = self.rnn(x, h0)\n        out = self.fc(out[:, -1, :])  # take last time step\n        return out\n```\n\n### Step 3: Train the Model\n\n* Loss function: Cross-Entropy Loss (for classification)\n* Optimizer: Adam / SGD\n* Feed input sequences → predict next element → compute loss → backpropagate\n\n\n\n## 5. Example Applications of RNN in Sequence Modeling\n\n| Application             | Input               | Output          |\n| ----------------------- | ------------------- | --------------- |\n| Language Modeling       | Words in a sentence | Next word       |\n| Time Series Forecasting | Stock prices        | Future price    |\n| Speech Recognition      | Audio frames        | Text transcript |\n| Machine Translation     | Source sentence     | Target sentence |\n| Music Generation        | Note sequence       | Next note       |\n\n\n\n## 6. Advantages of RNNs\n\n* Can process **arbitrary-length sequences**\n* Maintains **temporal dependencies** via hidden states\n* Suitable for sequential predictions\n\n\n\n## 7. Limitations\n\n* **Vanishing / exploding gradients** for long sequences\n* Hard to capture **long-range dependencies**\n* Training can be slow\n\n**Solution:** Use **LSTM** or **GRU** networks, which are variants of RNN that mitigate these issues.\n\n\n\n## 8. Visualization of RNN Unrolling\n\n```\nTime t=1       Time t=2       Time t=3       Time t=4\n  x1 → h1 → y1   x2 → h2 → y2   x3 → h3 → y3   x4 → h4 → y4\n         ↑           ↑            ↑            ↑\n         h0           h1           h2           h3  (hidden states carry memory)\n```\n\n* The hidden state (h_t) connects the network across time steps.\n\n\n\n## 9. Summary\n\n* **Sequence modeling** = predicting/understanding ordered data\n* **RNN** = neural network with memory (hidden state)\n* **Applications**: text, time series, speech, genomics\n* **Limitations**: vanishing gradient, long-range dependencies\n* **Improved variants**: LSTM, GRU","metadata":{}},{"cell_type":"markdown","source":"## Major problems of ANN\n- Variable size of input/output neurons\n- Too much computation\n- No parameter sharing","metadata":{}},{"cell_type":"markdown","source":"# **Types of Recurrent Neural Networks (RNNs)**\n\nRNNs come in **different architectures** depending on how they handle input, output, and sequence direction. Choosing the right type depends on the problem (e.g., prediction, classification, sequence generation).\n\n\n\n## 1. **Vanilla RNN (Simple RNN)**\n\n* **Structure:** Standard RNN with one hidden state per time step.\n* **Equations:**\n  [\n  h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\n  ]\n  [\n  y_t = W_{hy}h_t + b_y\n  ]\n* **Use case:** Simple sequence modeling tasks.\n* **Limitations:**\n\n  * Vanishing/exploding gradients\n  * Difficulty capturing long-term dependencies\n\n\n\n## 2. **Long Short-Term Memory (LSTM)**\n\n* **Structure:** RNN with **gates** to control information flow:\n\n  * **Forget gate** (f_t) – decides what to forget\n  * **Input gate** (i_t) – decides what to add to cell state\n  * **Output gate** (o_t) – decides what to output\n* **Cell state (C_t):** Maintains long-term memory\n* **Advantages:**\n\n  * Handles long-range dependencies\n  * Reduces vanishing gradient problem\n* **Use case:** Language modeling, machine translation, speech recognition\n\n\n\n## 3. **Gated Recurrent Unit (GRU)**\n\n* **Structure:** Simplified version of LSTM:\n\n  * Combines forget and input gates into **update gate**\n  * Uses **reset gate** to control memory update\n* **Advantages:**\n\n  * Fewer parameters than LSTM → faster training\n  * Often performs similarly to LSTM\n* **Use case:** Similar to LSTM, especially when computation efficiency is important\n\n\n\n## 4. **Bidirectional RNN (BiRNN)**\n\n* **Structure:** Processes sequence in **both directions**:\n\n  * Forward RNN: left → right\n  * Backward RNN: right → left\n* **Output:** Concatenates hidden states from both directions\n* **Advantages:** Captures **past and future context** for each time step\n* **Use case:** Part-of-speech tagging, named entity recognition, speech processing\n\n\n\n## 5. **Deep RNN**\n\n* **Structure:** Multiple RNN layers stacked on top of each other\n* **Advantages:** Can capture more **complex patterns**\n* **Use case:** Complex sequence modeling tasks like multi-layered language models\n\n\n\n## 6. **Echo State Networks (ESN)**\n\n* **Structure:** Sparse, randomly connected hidden layer (reservoir)\n* **Characteristic:** Only output weights are trained\n* **Use case:** Time series prediction, computationally efficient RNN\n\n\n\n## 7. **Recursive RNN (Tree-structured RNN)**\n\n* **Structure:** Works on **tree-structured data** instead of linear sequences\n* **Use case:** Natural language parsing, sentiment analysis\n\n---\n\n## 8. Summary Table\n\n| Type          | Key Feature                  | Pros                           | Cons                                    | Use Cases                   |\n| ------------- | ---------------------------- | ------------------------------ | --------------------------------------- | --------------------------- |\n| Vanilla RNN   | Simple hidden state          | Simple, easy to implement      | Vanishing gradient, short memory        | Basic sequences             |\n| LSTM          | Gates + cell state           | Long-term dependencies, stable | More parameters                         | NLP, speech, time series    |\n| GRU           | Update/reset gates           | Fewer parameters, fast         | Less expressive than LSTM in some tasks | NLP, time series            |\n| BiRNN         | Processes forward & backward | Uses past & future context     | Double computation                      | POS tagging, NER            |\n| Deep RNN      | Stacked layers               | Captures complex patterns      | Harder to train                         | Complex sequences           |\n| ESN           | Fixed random reservoir       | Fast training                  | Limited flexibility                     | Time series                 |\n| Recursive RNN | Tree-structured              | Works on hierarchies           | Complex                                 | Parsing, sentiment analysis |\n\n---\n\n## 9. Visualization Idea\n\n```\nVanilla RNN:   x1 → h1 → y1\n               x2 → h2 → y2\n               x3 → h3 → y3\n\nLSTM:          x1 → [Cell + Gates] → h1 → y1\n               x2 → [Cell + Gates] → h2 → y2\n\nGRU:           x1 → [Update/Reset Gates] → h1 → y1\n               x2 → [Update/Reset Gates] → h2 → y2\n\nBiRNN:         x1 → h1_forward →  \n                        ↘\n                         → y1\n               x1 → h1_backward →  \n```","metadata":{}},{"cell_type":"markdown","source":"# **RNN in Language Translation and Named Entity Recognition**\n\n\n## 1. RNN in Language Translation (Sequence-to-Sequence)\n\n**Problem:** Translate a sentence from **source language** (e.g., English) to **target language** (e.g., French).\n\n### a) Overview\n\n* Translation is a **sequence-to-sequence (Seq2Seq)** problem.\n* The input and output sequences are **not necessarily the same length**.\n* RNNs (often LSTM or GRU) are used in **encoder-decoder architecture**.\n\n\n### b) Encoder-Decoder Architecture\n\n**Encoder:**\n\n* Reads the **input sentence** word by word.\n* Updates hidden state (h_t) at each time step.\n* Final hidden state (h_T) is a **context vector** summarizing the entire input sentence.\n\n**Decoder:**\n\n* Initializes its hidden state with the encoder’s final hidden state.\n* Generates the **output sentence word by word**.\n* Uses the previous output word as input for the next time step during training (teacher forcing).\n\n```\nInput sentence: \"I am happy\"\nEncoder RNN:    x1 -> h1\n                x2 -> h2\n                x3 -> h3\nContext vector: h3\n\nDecoder RNN:    h3 + <START> -> y1: \"Je\"\n                h1 + y1 -> y2: \"suis\"\n                h2 + y2 -> y3: \"heureux\"\nOutput sentence: \"Je suis heureux\"\n```\n\n---\n\n### c) Attention Mechanism (Optional but Common)\n\n* Instead of encoding everything into **a single context vector**, attention allows the decoder to **focus on specific encoder states** at each time step.\n* This improves translation quality, especially for **long sentences**.\n\n\n\n### d) Key Steps in RNN Translation\n\n1. Tokenize input and output sentences → embeddings\n2. Pass input through **encoder RNN** → get hidden states\n3. Initialize **decoder RNN** with encoder’s hidden state\n4. Generate output sequence → predict each word\n5. Optionally apply **attention** over encoder hidden states\n6. Compute **loss** (cross-entropy between predicted and true words) → backpropagation\n\n\n\n### e) Advantages\n\n* Captures sequential dependencies in source language\n* Handles variable-length sequences\n* Can be extended with LSTM/GRU and attention\n\n\n\n## 2. RNN in Named Entity Recognition (NER)\n\n**Problem:** Identify entities (person, location, organization, etc.) in a text.\n\n**Example:**\n\n```\nInput sentence: \"Sanjan Acharya works at OpenAI.\"\nOutput: [\"Sanjan Acharya\" → PERSON, \"OpenAI\" → ORG]\n```\n\n---\n\n### a) How RNN Handles NER\n\n* NER is a **sequence labeling problem**.\n* Input: sequence of words\n* Output: sequence of labels (one per word)\n\n**RNN Approach:**\n\n1. Convert words to embeddings: `x1, x2, x3, ...`\n2. Pass embeddings through RNN:\n\n   ```\n   x1 -> h1 -> y1\n   x2 -> h2 -> y2\n   x3 -> h3 -> y3\n   ```\n3. Output `y_t` is the **predicted entity label** for word `t`:\n\n   * `B-PER` = beginning of a person name\n   * `I-PER` = inside person name\n   * `B-ORG`, `I-ORG`, `O` = outside any entity\n4. Use **softmax** on each `y_t` to get probabilities over entity classes\n5. Train with **cross-entropy loss** for sequence labeling\n\n\n\n### b) Bi-directional RNN in NER\n\n* Standard RNN reads left → right, but context on the **right** is also useful.\n* **BiRNN** or **BiLSTM** reads sequence both ways:\n\n  ```\n  Forward RNN: x1 -> h1f, x2 -> h2f, ...\n  Backward RNN: x3 -> h3b, x2 -> h2b, ...\n  Hidden state at t: h_t = concat(h_tf, h_tb)\n  ```\n* Each word prediction uses **past and future context**, improving entity recognition accuracy.\n\n\n\n### c) Example Workflow in NER\n\n```\nSentence: \"John lives in London\"\n\nWord embeddings: x1(\"John\"), x2(\"lives\"), x3(\"in\"), x4(\"London\")\n\nBiRNN hidden states: h1, h2, h3, h4\n\nSoftmax output:\ny1: B-PER\ny2: O\ny3: O\ny4: B-LOC\n```\n\n* Predicted entities: **John → PERSON**, **London → LOCATION**\n\n---\n\n## 3. Key Differences Between Translation and NER\n\n| Aspect       | Translation                         | NER                                |\n| ------------ | ----------------------------------- | ---------------------------------- |\n| Input        | Sequence of words                   | Sequence of words                  |\n| Output       | Sequence of words (target language) | Sequence of labels (entities)      |\n| Architecture | Encoder-Decoder RNN (+Attention)    | BiRNN / RNN with sequence labeling |\n| Goal         | Generate new sequence               | Tag each word                      |\n| Memory usage | Context vector or attention         | Hidden state per word              |\n\n---\n\n## 4. Summary\n\n* **RNN in Translation:** Encoder-decoder architecture, generates variable-length target sequence, optionally uses attention.\n* **RNN in NER:** Sequence labeling task, BiRNN recommended, predicts labels per word using contextual hidden states.\n* **Commonality:** Both rely on **temporal dependencies** captured by RNN hidden states.\n","metadata":{}},{"cell_type":"markdown","source":"# **Types of RNN Architectures**\n\nRNNs can be categorized based on the **relationship between input and output sequences**.\n\n\n\n## 1. **One-to-One RNN**\n\n* **Description:** Standard neural network.\n* **Input:** Single data point\n* **Output:** Single output\n* **Example:** Image classification\n\n```\nInput x → [RNN] → Output y\n```\n\n* **Use case:** When sequence modeling is **not required**; behaves like a feedforward network.\n\n\n\n## 2. **One-to-Many RNN**\n\n* **Description:** Single input produces a **sequence of outputs**.\n\n* **Example:** Image captioning\n\n* **Workflow:**\n\n  ```\n  Input x (image) → RNN → y1, y2, y3, ... (caption words)\n  ```\n\n* **Use case:** Generating sequences from a single input.\n\n\n\n## 3. **Many-to-One RNN**\n\n* **Description:** Sequence of inputs produces a **single output**.\n\n* **Example:** Sentiment analysis (predict sentiment from a sentence)\n\n* **Workflow:**\n\n  ```\n  x1 → h1\n  x2 → h2\n  x3 → h3\n          ↓\n          y (sentiment)\n  ```\n\n* **Use case:** Classification or regression of sequential data.\n\n\n\n## 4. **Many-to-Many RNN**\n\n* **Description:** Sequence of inputs produces a sequence of outputs.\n\n### a) **Equal Length Input-Output**\n\n* Input and output sequences have the **same length**\n* **Example:** Part-of-speech tagging, named entity recognition\n\n```\nx1 → y1\nx2 → y2\nx3 → y3\n```\n\n### b) **Variable Length Input-Output**\n\n* Input and output sequences can have **different lengths**\n* **Example:** Machine translation\n\n```\nInput: \"I am happy\"\nOutput: \"Je suis heureux\"\n```\n\n* Implemented using **Encoder-Decoder RNNs**, often with attention mechanism.\n\n---\n\n## 5. Summary Table\n\n| RNN Type                       | Input    | Output   | Example              |\n| ------------------------------ | -------- | -------- | -------------------- |\n| One-to-One                     | Single   | Single   | Image classification |\n| One-to-Many                    | Single   | Sequence | Image captioning     |\n| Many-to-One                    | Sequence | Single   | Sentiment analysis   |\n| Many-to-Many (equal length)    | Sequence | Sequence | NER, POS tagging     |\n| Many-to-Many (variable length) | Sequence | Sequence | Machine translation  |","metadata":{}}]}