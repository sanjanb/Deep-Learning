{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Sentence → Classification → Vector Embedding → Meaning → Contextual Meaning**\n\n#### **Contextual meaning:**\nUsed to capture how the same word can have different meanings in different contexts. This helps generate distinct vector embeddings for the same word depending on its usage and surrounding words.","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:39:51.847186Z","iopub.execute_input":"2025-11-02T12:39:51.847563Z","iopub.status.idle":"2025-11-02T12:39:51.852099Z","shell.execute_reply.started":"2025-11-02T12:39:51.847481Z","shell.execute_reply":"2025-11-02T12:39:51.851157Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import tensorflow_hub as hub\nimport tensorflow_text as text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:39:52.407650Z","iopub.execute_input":"2025-11-02T12:39:52.408236Z","iopub.status.idle":"2025-11-02T12:39:52.412937Z","shell.execute_reply.started":"2025-11-02T12:39:52.408207Z","shell.execute_reply":"2025-11-02T12:39:52.411668Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### Each encoder model has its own specific preprocessor, which must be used whenever performing any operation with that encoder.","metadata":{}},{"cell_type":"code","source":"preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\nencoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:39:53.276538Z","iopub.execute_input":"2025-11-02T12:39:53.276883Z","iopub.status.idle":"2025-11-02T12:39:53.281778Z","shell.execute_reply.started":"2025-11-02T12:39:53.276858Z","shell.execute_reply":"2025-11-02T12:39:53.280705Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"bert_preprocess_model = hub.KerasLayer(preprocess_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:40:38.179850Z","iopub.execute_input":"2025-11-02T12:40:38.180570Z","iopub.status.idle":"2025-11-02T12:40:47.425694Z","shell.execute_reply.started":"2025-11-02T12:40:38.180537Z","shell.execute_reply":"2025-11-02T12:40:47.424677Z"}},"outputs":[{"name":"stderr","text":"2025-11-02 12:40:41.305038: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"text_set = ['nice movie indeed', 'I love python programming', 'I got confused with typescript and javascript']\n\ntext_preprocess = bert_preprocess_model(text_set)\ntext_preprocess.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:47:54.973298Z","iopub.execute_input":"2025-11-02T12:47:54.973640Z","iopub.status.idle":"2025-11-02T12:47:55.180105Z","shell.execute_reply.started":"2025-11-02T12:47:54.973615Z","shell.execute_reply":"2025-11-02T12:47:55.179315Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_mask', 'input_type_ids', 'input_word_ids'])"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### **text_preprocess**, this object has different attributes, lets analyse one by one","metadata":{}},{"cell_type":"code","source":"text_preprocess['input_mask']\n\n# 128 is the maximum length of the vector that can be generated\n# 3 means for 3 sentences it generated 3 vector embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:52:53.408259Z","iopub.execute_input":"2025-11-02T12:52:53.408552Z","iopub.status.idle":"2025-11-02T12:52:53.416001Z","shell.execute_reply.started":"2025-11-02T12:52:53.408532Z","shell.execute_reply":"2025-11-02T12:52:53.414777Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3, 128), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"text_preprocess['input_type_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:53:59.252290Z","iopub.execute_input":"2025-11-02T12:53:59.252637Z","iopub.status.idle":"2025-11-02T12:53:59.260669Z","shell.execute_reply.started":"2025-11-02T12:53:59.252613Z","shell.execute_reply":"2025-11-02T12:53:59.259680Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3, 128), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"text_preprocess['input_word_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:54:11.047384Z","iopub.execute_input":"2025-11-02T12:54:11.047689Z","iopub.status.idle":"2025-11-02T12:54:11.054718Z","shell.execute_reply.started":"2025-11-02T12:54:11.047669Z","shell.execute_reply":"2025-11-02T12:54:11.053705Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3, 128), dtype=int32, numpy=\narray([[  101,  3835,  3185,  5262,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0],\n       [  101,  1045,  2293, 18750,  4730,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0],\n       [  101,  1045,  2288,  5457,  2007,  4127, 23235,  1998,  9262,\n        22483,   102,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0]], dtype=int32)>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"bert_model = hub.KerasLayer(encoder_url)\n\nbert_results = bert_model(text_preprocess)\nbert_results.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:57:28.472482Z","iopub.execute_input":"2025-11-02T12:57:28.472808Z","iopub.status.idle":"2025-11-02T12:58:05.118599Z","shell.execute_reply.started":"2025-11-02T12:57:28.472775Z","shell.execute_reply":"2025-11-02T12:58:05.117792Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"dict_keys(['default', 'encoder_outputs', 'pooled_output', 'sequence_output'])"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"bert_results['encoder_outputs']\n\n# output of each ecoder layer in the bert model\n# 3 -> sentences\n# 128 -> embedding\n# 768 -> sequence output\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:58:30.272444Z","iopub.execute_input":"2025-11-02T12:58:30.272808Z","iopub.status.idle":"2025-11-02T12:58:30.292243Z","shell.execute_reply.started":"2025-11-02T12:58:30.272745Z","shell.execute_reply":"2025-11-02T12:58:30.291272Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[<tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[ 0.12901433,  0.00644742, -0.03614973, ...,  0.04999627,\n           0.06149201, -0.02657528],\n         [ 1.1753385 ,  1.2140789 ,  1.1569982 , ...,  0.11634358,\n          -0.3585536 , -0.4049018 ],\n         [ 0.03859036,  0.5386998 , -0.21089786, ...,  0.21858224,\n           0.7260168 , -1.1158603 ],\n         ...,\n         [-0.0758702 , -0.25421908,  0.7075512 , ...,  0.50542   ,\n          -0.1887868 ,  0.15028337],\n         [-0.16066599, -0.28089687,  0.5759707 , ...,  0.52758545,\n          -0.11141391,  0.0288756 ],\n         [-0.04428162, -0.20279588,  0.59093565, ...,  0.8133835 ,\n          -0.39075804, -0.02601745]],\n \n        [[ 0.1890359 ,  0.02752538, -0.06513739, ..., -0.00620209,\n           0.15053897,  0.03165445],\n         [ 0.5916149 ,  0.75891405, -0.07240669, ...,  0.6190399 ,\n           0.82928926,  0.16161972],\n         [ 1.4460827 ,  0.4460263 ,  0.40990257, ...,  0.48255897,\n           0.6269115 ,  0.13463408],\n         ...,\n         [ 0.15147899, -0.21573858,  0.70329076, ..., -0.1253722 ,\n          -0.13787259,  0.27722064],\n         [ 0.05143806, -0.24052726,  0.5356912 , ..., -0.07915048,\n          -0.03307909,  0.17380913],\n         [ 0.2093471 , -0.1564526 ,  0.60395455, ...,  0.3290352 ,\n          -0.35827175,  0.08100393]],\n \n        [[ 0.07734597, -0.0128834 , -0.14733776, ...,  0.07448666,\n          -0.02434441,  0.00387298],\n         [ 0.59717846,  0.5097993 , -0.5344394 , ...,  0.5468299 ,\n           1.1238003 , -0.05803113],\n         [-0.64713544, -0.69625163,  0.39633676, ..., -0.44622934,\n           0.08692603,  0.6085682 ],\n         ...,\n         [-0.03711143, -0.43783402,  0.35453576, ...,  0.00392206,\n          -0.25633726,  0.16724877],\n         [-0.11747524, -0.4418528 ,  0.23369317, ...,  0.07090095,\n          -0.1602255 ,  0.05720298],\n         [ 0.03177388, -0.3802461 ,  0.2690187 , ...,  0.41764247,\n          -0.48791125, -0.03249798]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[ 1.41814910e-02, -2.20882505e-01, -1.50281817e-01, ...,\n           1.14156298e-01,  1.26180977e-01,  4.84339520e-02],\n         [ 1.20339024e+00,  1.34698689e+00,  1.70645344e+00, ...,\n           3.06105345e-01, -5.07425547e-01, -5.51474154e-01],\n         [ 4.21690941e-01,  8.11024427e-01, -2.56315768e-01, ...,\n          -7.72245973e-02,  8.93723726e-01, -1.44720566e+00],\n         ...,\n         [-1.90474659e-01, -2.38608494e-01,  8.14120293e-01, ...,\n           9.74936128e-01, -3.47742289e-01, -8.73358399e-02],\n         [-2.71509916e-01, -3.19849879e-01,  7.65938997e-01, ...,\n           9.67617154e-01, -2.95119345e-01, -1.57317847e-01],\n         [-2.13027552e-01, -1.92297071e-01,  7.33877838e-01, ...,\n           1.10404444e+00, -4.51028764e-01, -2.06830651e-01]],\n \n        [[ 8.97333771e-02, -1.84197068e-01, -1.66450843e-01, ...,\n           2.76133809e-02,  1.11877225e-01,  8.04172605e-02],\n         [ 5.83113611e-01,  5.95703244e-01,  3.60195428e-01, ...,\n           4.12701190e-01,  2.68092185e-01,  2.84005553e-01],\n         [ 2.11666727e+00,  5.17693698e-01,  8.63774538e-01, ...,\n           7.17872918e-01,  3.24050903e-01,  9.73997861e-02],\n         ...,\n         [ 2.43729115e-01, -5.77556677e-02,  6.84287906e-01, ...,\n           4.34836060e-01, -5.76609552e-01, -1.11310616e-01],\n         [ 1.68039113e-01, -3.09183449e-02,  5.86384535e-01, ...,\n           4.96258169e-01, -5.05679429e-01, -2.07821846e-01],\n         [ 2.48317569e-01,  3.15474719e-03,  5.15925288e-01, ...,\n           8.05023849e-01, -6.98996544e-01, -2.41864383e-01]],\n \n        [[-4.92494404e-02, -2.14715436e-01, -2.41055489e-01, ...,\n           1.85818315e-01,  1.26148492e-01,  2.06353143e-03],\n         [ 6.72579050e-01,  2.23514616e-01, -3.26037914e-01, ...,\n           8.40145871e-02,  7.00981975e-01, -3.46893162e-01],\n         [-4.83356476e-01, -6.81301281e-02,  6.74710155e-01, ...,\n          -5.15318155e-01,  1.53031826e-01,  4.79494423e-01],\n         ...,\n         [ 1.45734459e-01, -3.54224503e-01,  4.84384686e-01, ...,\n           5.55571377e-01, -6.95883751e-01,  2.81305425e-03],\n         [ 1.15732372e-01, -3.38122070e-01,  5.97862840e-01, ...,\n           6.32244289e-01, -7.34636009e-01, -2.75081135e-02],\n         [ 1.85989395e-01, -2.70505339e-01,  4.37297136e-01, ...,\n           9.41774547e-01, -1.01063919e+00, -2.53863275e-01]]],\n       dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[ 0.02275398, -0.27980238,  0.02345712, ...,  0.27867025,\n           0.11714803,  0.18175085],\n         [ 1.2574846 ,  0.8725082 ,  1.6266017 , ...,  0.4521091 ,\n          -0.8090258 , -0.5448977 ],\n         [ 0.75219864,  0.6357363 , -0.20566119, ..., -0.32381788,\n           0.7574951 , -1.4587923 ],\n         ...,\n         [-0.15107417, -0.21129039,  0.96894604, ...,  1.1261965 ,\n          -0.03214088, -0.22340299],\n         [-0.2812562 , -0.31140047,  0.84328926, ...,  1.1342679 ,\n          -0.08336602, -0.2516123 ],\n         [-0.2444908 , -0.21537894,  0.94809866, ...,  1.2419428 ,\n          -0.19873479, -0.33752584]],\n \n        [[ 0.10617061, -0.27990746, -0.01731809, ...,  0.20060441,\n           0.08148392,  0.21859078],\n         [ 0.6892587 ,  0.31591564,  0.5558663 , ...,  0.69039506,\n          -0.07141564,  0.41407174],\n         [ 2.5758884 ,  0.62520874,  1.2503715 , ...,  0.43957722,\n          -0.18525603, -0.05004874],\n         ...,\n         [ 0.20464341, -0.01561997,  0.83431435, ...,  0.80149585,\n          -0.12853877, -0.35842055],\n         [-0.02875639,  0.05097832,  0.6815927 , ...,  0.9003149 ,\n          -0.12434891, -0.42256355],\n         [ 0.13327701,  0.02257299,  0.77221745, ...,  1.0211186 ,\n          -0.30834723, -0.4562319 ]],\n \n        [[ 0.09088434, -0.2740692 , -0.04990475, ...,  0.12402862,\n           0.21814193,  0.13591154],\n         [ 0.87036824,  0.0285772 , -0.16654423, ...,  0.05816817,\n           0.6462586 , -0.18000823],\n         [-0.3619021 , -0.25705862,  0.4223001 , ..., -0.18432935,\n          -0.293855  ,  0.8214247 ],\n         ...,\n         [ 0.25384808, -0.5026137 ,  0.5338975 , ...,  0.7959932 ,\n          -0.07011385, -0.20706359],\n         [ 0.15416023, -0.4062801 ,  0.55567336, ...,  0.85700303,\n          -0.0777331 , -0.17310408],\n         [ 0.17682958, -0.39889812,  0.5599907 , ...,  1.0840186 ,\n          -0.33690542, -0.38256225]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[ 0.11484922, -0.64441115, -0.14245135, ...,  0.25474194,\n           0.00378413,  0.6110509 ],\n         [ 1.3035431 ,  0.7706048 ,  1.3185004 , ...,  0.35822502,\n          -0.6412162 , -0.32795328],\n         [ 1.1896739 ,  0.6279441 , -0.67501605, ..., -0.28870645,\n           0.47791922, -1.2806003 ],\n         ...,\n         [-0.22614762, -0.62762934,  1.0227482 , ...,  0.82923156,\n          -0.40314674,  0.04389237],\n         [-0.39901477, -0.75619066,  0.74893755, ...,  0.74922645,\n          -0.45077705, -0.00367936],\n         [-0.37728786, -0.7831045 ,  0.90572464, ...,  0.9736229 ,\n          -0.48285735, -0.07389838]],\n \n        [[ 0.15151384, -0.70752305, -0.2752042 , ...,  0.44345906,\n          -0.20920397,  0.49860165],\n         [ 0.8864791 , -0.24838138,  0.7353747 , ...,  0.7417942 ,\n          -0.13177215,  0.10158483],\n         [ 2.586581  ,  0.6188284 ,  0.5279886 , ...,  0.8487309 ,\n          -0.5915018 ,  0.02349873],\n         ...,\n         [-0.05526859, -0.43394667,  1.1783332 , ...,  0.91791654,\n          -0.4571815 , -0.2501464 ],\n         [-0.29328674, -0.23091221,  0.9938014 , ...,  1.0353785 ,\n          -0.42435732, -0.3494761 ],\n         [-0.15899095, -0.5009818 ,  0.9814213 , ...,  1.1373078 ,\n          -0.61888385, -0.44457805]],\n \n        [[ 0.26436466, -0.68783844, -0.24940649, ...,  0.28086877,\n           0.22336137,  0.53360534],\n         [ 1.0334786 , -0.12940712, -0.453167  , ..., -0.21714303,\n           0.34588632,  0.1868037 ],\n         [-0.82100713, -0.6921498 ,  0.35162422, ...,  0.30770364,\n          -0.60259753,  0.42307967],\n         ...,\n         [ 0.07614646, -0.990738  ,  0.6337902 , ...,  0.73355085,\n          -0.57051283, -0.18525395],\n         [-0.1117557 , -0.8513312 ,  0.69515604, ...,  0.7132566 ,\n          -0.5187891 , -0.05637923],\n         [-0.06595053, -0.86922234,  0.80591273, ...,  1.0353044 ,\n          -0.6761442 , -0.32800943]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-0.22174804, -0.42225346, -0.04924025, ..., -0.26284212,\n           0.06099934,  0.6379487 ],\n         [ 1.1627904 ,  0.6770536 ,  1.3072393 , ...,  0.22794664,\n          -0.54329526, -0.27383232],\n         [ 1.5308232 ,  0.72225475, -0.43608642, ...,  0.27435842,\n           0.24980265, -0.98753834],\n         ...,\n         [-0.17669767, -0.31260586,  1.0875314 , ...,  0.5924216 ,\n           0.01275356, -0.28171965],\n         [-0.3447377 , -0.42430058,  0.8798016 , ...,  0.5482611 ,\n          -0.08151908, -0.34001404],\n         [-0.4813991 , -0.31324273,  1.1702348 , ...,  0.8730986 ,\n          -0.07970177, -0.45325482]],\n \n        [[-0.21608128, -0.8986419 , -0.4499155 , ..., -0.08286179,\n          -0.17226869,  0.66199833],\n         [ 0.66614497, -0.54916906,  0.46460572, ...,  0.17649351,\n           0.22822523,  0.34962183],\n         [ 2.062962  ,  0.6991978 ,  0.35604504, ...,  0.5295332 ,\n          -0.34809294,  0.00831391],\n         ...,\n         [-0.11594022, -0.17149246,  0.89943963, ...,  0.6299016 ,\n          -0.33990237, -0.2004923 ],\n         [-0.13007513, -0.02739116,  0.7079971 , ...,  0.8059462 ,\n          -0.30724147, -0.19956325],\n         [-0.2631489 , -0.26137114,  0.63385624, ...,  0.8150187 ,\n          -0.4540036 , -0.35120687]],\n \n        [[-0.03374526, -0.8151952 , -0.17993191, ..., -0.05595421,\n           0.02851446,  0.4012487 ],\n         [ 0.92064923, -0.46038967, -0.28778505, ..., -0.07119651,\n           0.09453101,  0.26253703],\n         [-1.104799  , -1.2248849 ,  0.14355765, ..., -0.02341479,\n          -0.26508513,  0.41849574],\n         ...,\n         [ 0.10215107, -0.51457983,  0.14290074, ...,  0.4634572 ,\n          -0.39531487, -0.35152858],\n         [-0.03308252, -0.5460905 ,  0.2061806 , ...,  0.4056768 ,\n          -0.47479504, -0.2491402 ],\n         [-0.03037647, -0.584792  ,  0.38810351, ...,  0.67902243,\n          -0.57235813, -0.48370698]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-0.03392693, -0.39431506,  0.04223732, ..., -0.07905565,\n           0.01993829,  0.7692066 ],\n         [ 1.4294362 , -0.08699046,  1.5299488 , ...,  0.22512163,\n          -1.0060078 , -0.23702237],\n         [ 1.5031742 ,  0.6725792 , -0.5259017 , ...,  0.10906949,\n           0.27042392, -1.2567508 ],\n         ...,\n         [-0.3003356 , -0.02261963,  1.318741  , ...,  0.69288754,\n          -0.09798431, -0.17419742],\n         [-0.43754166, -0.21149263,  1.1094568 , ...,  0.5006598 ,\n          -0.17692392, -0.18529142],\n         [-0.65920526, -0.19607908,  1.313407  , ...,  0.7320075 ,\n          -0.1996736 , -0.32469165]],\n \n        [[-0.2733466 , -0.9526846 , -0.78696615, ..., -0.12205317,\n          -0.08783774,  0.75811654],\n         [ 0.46756995, -0.16237512, -0.01155872, ...,  0.1878125 ,\n           0.6216474 ,  0.03009908],\n         [ 1.79474   ,  0.9051074 ,  0.08580908, ...,  0.77093554,\n          -0.6682788 , -0.06755506],\n         ...,\n         [-0.18337466, -0.15806574,  1.1826942 , ...,  0.83047533,\n          -0.39406464, -0.23047718],\n         [-0.22270177,  0.00975942,  0.93408376, ...,  1.1286676 ,\n          -0.34884351, -0.19491422],\n         [-0.3038619 , -0.29683158,  0.8247157 , ...,  1.0090815 ,\n          -0.46407658, -0.31465593]],\n \n        [[ 0.08375082, -0.8779652 , -0.34643945, ..., -0.08595748,\n          -0.04680476,  0.40233204],\n         [ 0.75154823, -0.23233299, -0.55583006, ..., -0.3475439 ,\n          -0.39879844, -0.0771303 ],\n         [-1.2187839 , -0.8877883 , -0.0511571 , ...,  0.04247461,\n          -0.69412553, -0.27390707],\n         ...,\n         [ 0.06409355, -0.25445518,  0.6108881 , ...,  0.45380735,\n          -0.420007  , -0.34188217],\n         [-0.00591953, -0.25896716,  0.632068  , ...,  0.51711935,\n          -0.50747293, -0.21264817],\n         [-0.03298619, -0.35153034,  0.7330184 , ...,  0.66169   ,\n          -0.57544893, -0.4405901 ]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-0.23803474, -0.66554064,  0.29917186, ...,  0.03415125,\n           0.29362822,  0.965363  ],\n         [ 1.5040759 ,  0.00695055,  1.4314135 , ...,  0.24628793,\n          -0.5521965 , -0.32215467],\n         [ 1.3629025 ,  0.4774174 , -0.61529875, ...,  0.08586986,\n           0.43922517, -1.5594157 ],\n         ...,\n         [-0.16863957,  0.00209697,  1.3201773 , ...,  0.94280356,\n          -0.04737277,  0.21490341],\n         [-0.26666403, -0.2488125 ,  1.221326  , ...,  0.6352798 ,\n          -0.0444242 ,  0.04466234],\n         [-0.53366196, -0.35796076,  1.3447868 , ...,  0.60384405,\n          -0.08427155, -0.15712021]],\n \n        [[-0.05967472, -0.8448214 , -0.8939021 , ..., -0.10863932,\n           0.40932417,  0.79408616],\n         [ 0.5440508 , -0.31962895, -0.45491505, ...,  0.48068678,\n           0.77862483,  0.22774255],\n         [ 1.4479868 ,  1.1230615 ,  0.09567937, ...,  1.3554294 ,\n           0.06143552, -0.00836905],\n         ...,\n         [ 0.25652316, -0.14106539,  1.1854341 , ...,  0.9568372 ,\n          -0.21603955, -0.06946395],\n         [ 0.2581914 ,  0.07949172,  0.9851643 , ...,  1.051822  ,\n          -0.07890932, -0.12841499],\n         [-0.0526895 , -0.33431396,  0.9637336 , ...,  0.8960701 ,\n          -0.10082396, -0.28129607]],\n \n        [[ 0.13753691, -0.96091413, -0.65133905, ..., -0.12559974,\n           0.5091605 ,  0.7492668 ],\n         [ 0.71635664, -0.3451249 , -0.53636706, ...,  0.08335134,\n          -0.11780682,  0.06275522],\n         [-1.0292779 , -0.98003113,  0.16318367, ...,  0.03676886,\n          -0.1357518 , -0.35986745],\n         ...,\n         [ 0.5854891 , -0.38117722,  0.661042  , ...,  0.6115004 ,\n          -0.21812356, -0.10559782],\n         [ 0.48137286, -0.37423685,  0.7539623 , ...,  0.7043333 ,\n          -0.23801371,  0.10848051],\n         [ 0.36255917, -0.48079422,  0.9170559 , ...,  0.68893385,\n          -0.3564898 , -0.22086444]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-7.08268583e-02, -2.56898910e-01, -3.59632000e-02, ...,\n          -3.58813107e-01,  1.35247707e-01,  1.01361156e+00],\n         [ 9.74719644e-01, -1.97129548e-01,  1.43180776e+00, ...,\n          -1.40520334e-01, -5.04318699e-02, -2.71415226e-02],\n         [ 1.12733972e+00,  2.71227241e-01, -2.86229014e-01, ...,\n           9.33858305e-02,  2.99082965e-01, -1.37644434e+00],\n         ...,\n         [-9.57861915e-02,  2.91807860e-01,  1.58791554e+00, ...,\n           8.50042164e-01, -5.12837395e-02,  2.03400433e-01],\n         [-2.22688198e-01, -8.38959664e-02,  1.72560108e+00, ...,\n           6.12221956e-01,  1.09766185e-01,  7.00463504e-02],\n         [-5.78300655e-01, -3.59156519e-01,  1.60359526e+00, ...,\n           3.41897875e-01,  5.28810099e-02, -1.54009238e-01]],\n \n        [[ 6.10169731e-02, -3.51616263e-01, -8.48914623e-01, ...,\n          -4.85549927e-01,  4.27030951e-01,  6.38290346e-01],\n         [ 6.63764834e-01,  1.82997435e-04, -7.90788412e-01, ...,\n           3.06870073e-01,  7.58436620e-01,  7.32401252e-01],\n         [ 1.44946980e+00,  1.12650406e+00,  1.77092046e-01, ...,\n           2.34715462e-01,  3.49294722e-01,  4.37097549e-01],\n         ...,\n         [ 3.23395580e-01,  1.75758511e-01,  8.76723289e-01, ...,\n           9.89041626e-01, -2.70508140e-01, -3.08094114e-01],\n         [ 4.04042691e-01,  5.13649106e-01,  7.27480829e-01, ...,\n           9.49421287e-01, -1.02156401e-02, -3.46442819e-01],\n         [-9.13090073e-03, -8.70166421e-02,  7.30317712e-01, ...,\n           8.54397476e-01, -1.36612192e-01, -4.74935412e-01]],\n \n        [[ 4.48836595e-01, -6.87025845e-01, -6.76429689e-01, ...,\n          -3.39534044e-01,  4.75651771e-01,  6.85257256e-01],\n         [ 5.21472454e-01, -3.46962273e-01, -8.51714015e-01, ...,\n          -6.83140382e-02,  9.55991820e-02,  3.42002302e-01],\n         [-8.47991168e-01, -8.06507111e-01,  3.82396430e-01, ...,\n           9.77389812e-02,  2.66285002e-01,  9.12548825e-02],\n         ...,\n         [ 6.41590595e-01, -2.27676988e-01,  7.32046366e-01, ...,\n           7.11258292e-01, -3.78358752e-01, -2.40067616e-01],\n         [ 5.48015714e-01, -1.61680177e-01,  8.93350601e-01, ...,\n           7.27360308e-01, -4.02878702e-01, -1.05447220e-02],\n         [ 3.70100588e-01, -3.34031403e-01,  1.00137663e+00, ...,\n           7.41134465e-01, -4.83968616e-01, -2.91627049e-01]]],\n       dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-0.09403507, -0.05987601, -0.01171748, ..., -0.22167565,\n          -0.07650115,  0.5765812 ],\n         [ 0.7748069 ,  0.15188183,  1.0920005 , ..., -0.19562374,\n           0.15173177, -0.01071361],\n         [ 0.851913  ,  0.25842658, -0.7703554 , ...,  0.03356688,\n           0.3401043 , -1.391868  ],\n         ...,\n         [-0.19780222,  0.5281968 ,  0.9023549 , ...,  0.27919748,\n          -0.28763872,  0.75109136],\n         [-0.35067368,  0.09291886,  1.3383355 , ...,  0.2261968 ,\n          -0.03588904,  0.2734386 ],\n         [-0.9044508 , -0.20991236,  1.1977335 , ...,  0.3511848 ,\n          -0.21743627,  0.01482281]],\n \n        [[-0.07011157, -0.02392504, -0.61924326, ..., -0.14542049,\n           0.34049398,  0.4122478 ],\n         [ 0.59261596,  0.19015962, -0.37399572, ...,  0.37239635,\n           0.39152876,  0.42580426],\n         [ 1.1394773 ,  0.84398586,  0.38893992, ...,  0.20898244,\n           0.25176343,  0.2639999 ],\n         ...,\n         [ 0.43305993,  0.4201215 ,  1.09651   , ...,  1.1872365 ,\n          -0.16672105,  0.00856193],\n         [ 0.5754229 ,  0.86609393,  1.1184685 , ...,  1.0475053 ,\n           0.0426427 , -0.01540364],\n         [-0.05488031, -0.07306911,  0.9012205 , ...,  0.8004526 ,\n          -0.16820663, -0.3550739 ]],\n \n        [[ 0.26661876, -0.4568149 , -0.7089352 , ..., -0.29381293,\n           0.56612575,  0.53085506],\n         [ 0.26048735, -0.39955494, -0.4628594 , ..., -0.22446883,\n           0.0823227 , -0.05291704],\n         [-0.81412446, -1.0098288 ,  0.7838518 , ..., -0.01378283,\n           0.21797118,  0.05441342],\n         ...,\n         [ 0.79827523, -0.28646228,  0.62093174, ...,  0.7780726 ,\n          -0.14941302, -0.2980069 ],\n         [ 0.7426957 , -0.12920094,  0.8644797 , ...,  0.8403563 ,\n          -0.26219228,  0.0720859 ],\n         [ 0.45430058, -0.3991578 ,  1.0921233 , ...,  0.80294174,\n          -0.31966496, -0.30958408]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-0.22671334,  0.03110908,  0.3218608 , ..., -0.28090188,\n          -0.75050807,  0.4455796 ],\n         [ 0.7712556 ,  0.11217897,  0.66033226, ..., -0.09223787,\n           0.34821138, -0.3690504 ],\n         [ 0.5122406 , -0.1992901 , -0.84775007, ..., -0.28812042,\n           0.2238371 , -1.21246   ],\n         ...,\n         [ 0.12500864,  0.6663391 ,  0.98522127, ..., -0.08709332,\n          -0.5805445 ,  0.6127215 ],\n         [ 0.01400362,  0.10471095,  1.2684823 , ..., -0.01833049,\n          -0.35933125,  0.15463182],\n         [-0.80938363, -0.36068112,  1.3501263 , ...,  0.5893608 ,\n          -0.50932384, -0.09766904]],\n \n        [[-0.21535009, -0.23962244, -0.30932364, ..., -0.25964975,\n           0.09695325,  0.41826698],\n         [ 0.5070063 , -0.01544834, -0.10519649, ...,  0.38675362,\n           0.22041132, -0.10293254],\n         [ 1.0366052 ,  0.91713095,  0.26164168, ...,  0.33182096,\n           0.45597035,  0.1269785 ],\n         ...,\n         [ 0.161906  ,  0.76270115,  1.2737674 , ...,  0.5726001 ,\n          -0.05929493,  0.2622408 ],\n         [ 0.46124482,  1.1673867 ,  0.959477  , ...,  0.4603301 ,\n           0.1352138 ,  0.45136017],\n         [-0.24184078,  0.17352736,  0.8415638 , ...,  0.24766803,\n          -0.14810285,  0.06557381]],\n \n        [[ 0.06255893, -0.5805526 , -0.5708255 , ..., -0.416542  ,\n           0.10878696,  0.3831417 ],\n         [ 0.13428831, -1.0350919 , -0.20397013, ..., -0.85177195,\n           0.04201371, -0.33182967],\n         [-0.60359067, -1.280341  ,  0.63942105, ..., -0.36408013,\n          -0.2260286 , -0.35625526],\n         ...,\n         [ 0.5281307 , -0.49198452,  0.60262483, ...,  0.39561993,\n          -0.37073427, -0.16954973],\n         [ 0.47732627, -0.22202368,  0.67091733, ...,  0.3698484 ,\n          -0.41312847,  0.15981166],\n         [ 0.2781809 , -0.44110215,  0.8941237 , ...,  0.4765302 ,\n          -0.51503146, -0.2956219 ]]], dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[-2.19515972e-02,  2.13035733e-01,  3.11359018e-01, ...,\n          -2.39179000e-01, -2.78708667e-01,  2.04084501e-01],\n         [ 8.30984235e-01,  1.90780744e-01,  7.11346090e-01, ...,\n          -3.00244242e-01,  3.11524063e-01, -2.59339601e-01],\n         [ 3.73089314e-01, -4.22126234e-01, -6.66908681e-01, ...,\n          -4.52976167e-01,  3.20451796e-01, -2.99885988e-01],\n         ...,\n         [ 3.31666410e-01,  7.18358994e-01,  8.99610400e-01, ...,\n          -3.49144936e-01, -2.99868464e-01,  5.23886085e-01],\n         [ 2.38532394e-01,  2.04842612e-01,  1.13064301e+00, ...,\n          -1.51129216e-01, -1.37811288e-01,  7.69989341e-02],\n         [-3.84120733e-01, -3.81587535e-01,  1.33972728e+00, ...,\n           5.77555597e-01, -1.55400291e-01, -2.92945355e-01]],\n \n        [[ 1.43630147e-01,  1.69801340e-01,  4.51200530e-02, ...,\n          -6.21678680e-02, -1.57528929e-02,  2.87870258e-01],\n         [ 5.84760189e-01,  2.69933105e-01, -2.85206527e-01, ...,\n           3.38931262e-01,  1.17773056e-01,  3.69869322e-02],\n         [ 1.25257814e+00,  1.25564456e+00,  3.87543887e-01, ...,\n           1.72757581e-01,  4.96662140e-01,  6.13780916e-01],\n         ...,\n         [ 1.95486128e-01,  4.40819353e-01,  1.03893638e+00, ...,\n           1.45270333e-01, -2.77438790e-01,  1.90321833e-01],\n         [ 4.48164821e-01,  7.64477134e-01,  6.98005080e-01, ...,\n          -8.35930929e-04, -7.96444789e-02,  4.93358552e-01],\n         [-2.33981058e-01, -2.03336254e-01,  3.48247170e-01, ...,\n          -6.43424988e-02, -3.20875883e-01,  2.23340355e-02]],\n \n        [[ 4.00340855e-02, -2.60326177e-01, -3.92364681e-01, ...,\n          -4.16531205e-01,  2.71957964e-01,  3.45632046e-01],\n         [ 1.83569610e-01, -8.58318746e-01, -3.42939645e-01, ...,\n          -7.04160333e-01,  2.37213969e-01,  4.47240993e-02],\n         [-5.79710305e-01, -9.75337267e-01,  4.83809173e-01, ...,\n          -6.43775582e-01,  4.39508647e-01,  7.05506280e-02],\n         ...,\n         [ 4.52879906e-01, -7.76432574e-01,  4.64795768e-01, ...,\n           1.03952944e-01, -3.48059982e-01, -4.01672989e-01],\n         [ 4.15762186e-01, -5.41250169e-01,  5.16142666e-01, ...,\n          -2.80503444e-02, -4.21119243e-01, -1.21153407e-01],\n         [ 2.04563245e-01, -6.78810418e-01,  6.52935565e-01, ...,\n           1.46635979e-01, -4.24393952e-01, -4.37932581e-01]]],\n       dtype=float32)>,\n <tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\n array([[[ 0.07292046,  0.08567815,  0.14476833, ..., -0.09677089,\n           0.08722138,  0.07711098],\n         [ 0.17839392, -0.19006105,  0.50349474, ..., -0.05869799,\n           0.32717094, -0.15578488],\n         [ 0.18701468, -0.43388787, -0.48875138, ..., -0.15502772,\n           0.00145124, -0.24470966],\n         ...,\n         [ 0.12083106,  0.12884258,  0.46453524, ...,  0.07375515,\n           0.17441994,  0.16522008],\n         [ 0.07967864, -0.01190711,  0.50225407, ...,  0.13777758,\n           0.21002217,  0.00624588],\n         [-0.07212694, -0.2830348 ,  0.59033334, ...,  0.47551954,\n           0.16668473, -0.08920345]],\n \n        [[-0.07900596,  0.36335123, -0.2110157 , ..., -0.17183766,\n           0.1629974 ,  0.6724269 ],\n         [ 0.27883503,  0.43716323, -0.35764694, ..., -0.0446365 ,\n           0.3831516 ,  0.5887988 ],\n         [ 1.203767  ,  1.0727018 ,  0.4840876 , ...,  0.24920997,\n           0.40730938,  0.404818  ],\n         ...,\n         [ 0.08630033,  0.19353835,  0.47540033, ...,  0.18880166,\n          -0.06474119,  0.3131858 ],\n         [ 0.1588704 ,  0.2857266 ,  0.37340793, ...,  0.09309113,\n          -0.0496955 ,  0.38761106],\n         [-0.08079889, -0.09572859,  0.26809785, ...,  0.13979614,\n          -0.0631586 ,  0.27288333]],\n \n        [[ 0.0824596 ,  0.12162127, -0.26337087, ..., -0.31238094,\n           0.3972744 ,  0.69767565],\n         [ 0.10951686, -0.5061657 , -0.4181754 , ..., -0.4419826 ,\n           0.6780484 ,  0.28901199],\n         [-0.35846493, -0.65292305,  0.3443018 , ..., -0.24278376,\n           0.327025  , -0.00386933],\n         ...,\n         [ 0.19437917, -0.30761975,  0.39026532, ...,  0.10080706,\n          -0.02257729, -0.00955789],\n         [ 0.17897038, -0.17200962,  0.40754163, ...,  0.08311715,\n          -0.05209567,  0.10770501],\n         [ 0.08996132, -0.22507383,  0.48002392, ...,  0.08921923,\n          -0.08130187, -0.03252142]]], dtype=float32)>]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"bert_results['sequence_output']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:00:21.412569Z","iopub.execute_input":"2025-11-02T13:00:21.412918Z","iopub.status.idle":"2025-11-02T13:00:21.420572Z","shell.execute_reply.started":"2025-11-02T13:00:21.412892Z","shell.execute_reply":"2025-11-02T13:00:21.419626Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3, 128, 768), dtype=float32, numpy=\narray([[[ 0.07292046,  0.08567815,  0.14476833, ..., -0.09677089,\n          0.08722138,  0.07711098],\n        [ 0.17839392, -0.19006105,  0.50349474, ..., -0.05869799,\n          0.32717094, -0.15578488],\n        [ 0.18701468, -0.43388787, -0.48875138, ..., -0.15502772,\n          0.00145124, -0.24470966],\n        ...,\n        [ 0.12083106,  0.12884258,  0.46453524, ...,  0.07375515,\n          0.17441994,  0.16522008],\n        [ 0.07967864, -0.01190711,  0.50225407, ...,  0.13777758,\n          0.21002217,  0.00624588],\n        [-0.07212694, -0.2830348 ,  0.59033334, ...,  0.47551954,\n          0.16668473, -0.08920345]],\n\n       [[-0.07900596,  0.36335123, -0.2110157 , ..., -0.17183766,\n          0.1629974 ,  0.6724269 ],\n        [ 0.27883503,  0.43716323, -0.35764694, ..., -0.0446365 ,\n          0.3831516 ,  0.5887988 ],\n        [ 1.203767  ,  1.0727018 ,  0.4840876 , ...,  0.24920997,\n          0.40730938,  0.404818  ],\n        ...,\n        [ 0.08630033,  0.19353835,  0.47540033, ...,  0.18880166,\n         -0.06474119,  0.3131858 ],\n        [ 0.1588704 ,  0.2857266 ,  0.37340793, ...,  0.09309113,\n         -0.0496955 ,  0.38761106],\n        [-0.08079889, -0.09572859,  0.26809785, ...,  0.13979614,\n         -0.0631586 ,  0.27288333]],\n\n       [[ 0.0824596 ,  0.12162127, -0.26337087, ..., -0.31238094,\n          0.3972744 ,  0.69767565],\n        [ 0.10951686, -0.5061657 , -0.4181754 , ..., -0.4419826 ,\n          0.6780484 ,  0.28901199],\n        [-0.35846493, -0.65292305,  0.3443018 , ..., -0.24278376,\n          0.327025  , -0.00386933],\n        ...,\n        [ 0.19437917, -0.30761975,  0.39026532, ...,  0.10080706,\n         -0.02257729, -0.00955789],\n        [ 0.17897038, -0.17200962,  0.40754163, ...,  0.08311715,\n         -0.05209567,  0.10770501],\n        [ 0.08996132, -0.22507383,  0.48002392, ...,  0.08921923,\n         -0.08130187, -0.03252142]]], dtype=float32)>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# BERT on TensorFlow Hub — Step-by-step explanation\n\n\n\n## 1. Goal\n\nUnderstand what each TensorFlow Hub BERT artifact does and how to interpret the tensors returned by the preprocessing layer and the encoder. After reading this you should know:\n\n* What `bert_preprocess_model` returns (`input_word_ids`, `input_mask`, `input_type_ids`).\n* What the BERT encoder outputs (`sequence_output`, `pooled_output`, `encoder_outputs`, `default`).\n* How to use `sequence_output` vs `pooled_output` for different downstream tasks (classification, token labeling, retrieval, etc.).\n\n\n## 2. Links (useful references)\n\n* List of BERT models on TF Hub: [https://tfhub.dev/google/collections/bert/1](https://tfhub.dev/google/collections/bert/1)\n* BERT (uncased, base) info: [https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4)\n\n\n## 3. Minimal code (your snippet)\n\n```python\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\npreprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\nencoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n\nbert_preprocess_model = hub.KerasLayer(preprocess_url)\ntext_test = ['nice movie indeed','I love python programming']\ntext_preprocessed = bert_preprocess_model(text_test)\ntext_preprocessed.keys()\n# dict_keys(['input_mask', 'input_type_ids', 'input_word_ids'])\n```\n\n> The preprocessing layer tokenizes text, adds special tokens, and returns integer tensors ready for the BERT encoder.\n\n\n\n## 4. What the preprocessing outputs mean\n\nWhen you print `text_preprocessed.keys()` you get three tensors:\n\n* `input_word_ids` — token ids (integers) for each token in each input sequence. These are the tokenized IDs the encoder expects. The sequences are padded to a fixed max length (here 128).\n\n* `input_mask` — a mask with `1` for real tokens and `0` for padding tokens. This tells the model which positions are meaningful.\n\n* `input_type_ids` (also called `segment ids`) — used to distinguish sentence A vs sentence B when BERT is used for paired-sentence tasks (e.g., question-answer, next-sentence prediction). For single-sentence inputs this is usually all zeros.\n\nExample shapes from your run:\n\n* Each tensor has shape `(batch_size, max_seq_len)` → here `(2, 128)`.\n* `input_word_ids` contains special token ids at the start and end: `101` is `[CLS]` and `102` is `[SEP]` in the WordPiece vocabulary used by BERT.\n\n> Quick note: `101` (CLS) is placed at the beginning of every input. `102` (SEP) ends a single sentence or separates sentence A and B.\n\n\n## 5. Load the encoder and inspect outputs\n\n```python\nbert_model = hub.KerasLayer(encoder_url)\nbert_results = bert_model(text_preprocessed)\nbert_results.keys()\n# dict_keys(['default', 'encoder_outputs', 'pooled_output', 'sequence_output'])\n```\n\nThese keys are common across TF Hub BERT models. Here's what each entry contains:\n\n* `sequence_output` — a tensor of shape `(batch_size, seq_len, hidden_size)` (here `(2, 128, 768)`). It contains the contextualized embedding vector for **every token** in the input sequence. Use this for token-level tasks like NER, POS tagging, or any task where you need a vector per token.\n\n* `pooled_output` — a tensor of shape `(batch_size, hidden_size)` (here `(2, 768)`). This is typically the embedding corresponding to the `[CLS]` token after an extra pooling step (in the original BERT implementation it's passed through a small tanh activation layer). Use this as a fixed-length sentence embedding for classification tasks (sentiment, intent, etc.).\n\n* `encoder_outputs` — a list (or tuple) of length `L` where each element is the sequence output from that encoder layer. For BERT base, `len(encoder_outputs) == 12`. Each element has shape `(batch_size, seq_len, hidden_size)` and corresponds to the output after each Transformer block. The last item in this list equals `sequence_output`.\n\n* `default` — some TF Hub models include a `default` key that is often the most convenient top-level object (it commonly maps to the pooled output or another default tensor). Check model docs for exact behaviour; prefer explicit keys (`sequence_output`, `pooled_output`) for clarity.\n\nYou observed:\n\n```python\nbert_results['encoder_outputs'][-1] == bert_results['sequence_output']\n# returns True (element-wise equality)\n```\n\nThat shows the final encoder layer output is the same tensor as `sequence_output`.\n\n\n## 6. How to use these outputs in common tasks\n\n* **Sentence classification (sentiment, intent, etc.)**\n\n  * Use `pooled_output` as a feature vector. Feed it to one or more Dense layers plus a softmax/sigmoid head.\n  * Example: `Dense(256, activation='relu')(pooled_output) -> Dense(num_classes, activation='softmax')`\n\n* **Token classification (NER, chunking)**\n\n  * Use `sequence_output` to get a vector per token. Pass that through a `TimeDistributed(Dense(...))` or a token-level classifier.\n  * Remember to ignore padded tokens using `input_mask` when computing loss and metrics.\n\n* **Semantic search / embeddings**\n\n  * You can use `pooled_output` or a custom pooling over `sequence_output` (mean pooling over non-padded tokens) to get sentence embeddings. If you need strong semantic embeddings, consider fine-tuning or using models trained specifically for sentence embeddings.\n\n* **Layer-wise features**\n\n  * Use `encoder_outputs[i]` to experiment: lower layers capture syntactic features, middle layers mix syntax/semantics, and higher layers capture higher-level semantics useful for classification. Some practitioners concatenate or average several top layers for improved performance.\n\n\n## 7. Small code snippets you’ll likely use\n\n**Get CLS embedding (pooled) for classification**\n\n```python\ncls_embeddings = bert_results['pooled_output']  # shape: (batch_size, hidden_size)\n```\n\n**Get token vectors for sequence labeling**\n\n```python\ntoken_embeddings = bert_results['sequence_output']  # shape: (batch_size, seq_len, hidden_size)\n```\n\n**Mask-aware mean pooling across tokens**\n\n```python\nimport tensorflow as tf\nmask = tf.cast(text_preprocessed['input_mask'], tf.float32)  # shape (batch, seq_len)\nemb = bert_results['sequence_output']  # (batch, seq_len, hidden_size)\nmasked_emb = emb * tf.expand_dims(mask, axis=-1)\nsum_emb = tf.reduce_sum(masked_emb, axis=1)\nlengths = tf.reduce_sum(mask, axis=1, keepdims=True)\nmean_pooled = sum_emb / tf.maximum(lengths, 1.0)\n# mean_pooled has shape (batch, hidden_size)\n```\n\n**Use encoder layer outputs**\n\n```python\n# encoder_outputs is an iterable of layer outputs\nall_layers = bert_results['encoder_outputs']\n# all_layers[i].shape == (batch, seq_len, hidden_size)\n```\n\n\n## 8. Practical tips and gotchas\n\n* **Use the matching preprocessor for a model.** The tokenizer/vocabulary and preprocessing assumptions must match the encoder (WordPiece vocabulary, casing). The preprocessor you used (`bert_en_uncased_preprocess/3`) matches the uncased encoder.\n\n* **Max sequence length and padding.** Default preprocessors pad or truncate to a fixed `max_seq_length` (128 here). If your task needs longer contexts, choose a model and preprocessor that support longer lengths or handle long text by chunking.\n\n* **Fine-tuning vs. feature-extraction.** For best performance on a downstream task, fine-tune the encoder (unfreeze gradients). For quick experiments, using `pooled_output` as a frozen feature is simpler.\n\n* **Token alignment.** If you need to map original words to tokens (for token-level labeling), remember WordPiece may split words into sub-tokens. Decide on a strategy: map labels to the first sub-token, average sub-token vectors, or use the original tokenizer to align.\n\n* **Versioning.** TF Hub modules have versions. If reproducibility is important, pin the exact module version (you already did by using `/4` and `/3`).\n\n---\n\n## 9. Summary (short)\n\n* Preprocessing converts raw text into `input_word_ids`, `input_mask`, and `input_type_ids`.\n* The encoder returns token-level vectors (`sequence_output`), a pooled sentence vector (`pooled_output`), and intermediate layer outputs (`encoder_outputs`).\n* Use `pooled_output` for sentence-level tasks and `sequence_output` for token-level tasks. `encoder_outputs` lets you inspect representations from every Transformer layer.\n","metadata":{}}]}