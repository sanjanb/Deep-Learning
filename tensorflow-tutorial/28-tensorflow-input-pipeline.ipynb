{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:32:10.664055Z","iopub.execute_input":"2025-10-13T13:32:10.664386Z","iopub.status.idle":"2025-10-13T13:32:10.669866Z","shell.execute_reply.started":"2025-10-13T13:32:10.664361Z","shell.execute_reply":"2025-10-13T13:32:10.668551Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"daily_sales_number = [21, 22, -108, 31, -1, 32, 34, 31]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:32:10.808014Z","iopub.execute_input":"2025-10-13T13:32:10.808347Z","iopub.status.idle":"2025-10-13T13:32:10.812928Z","shell.execute_reply.started":"2025-10-13T13:32:10.808322Z","shell.execute_reply":"2025-10-13T13:32:10.811922Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import tensorflow as tf\n\ntf_dataset = tf.data.Dataset.from_tensor_slices(daily_sales_number)\n# this data is encoded in object","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:32:10.943096Z","iopub.execute_input":"2025-10-13T13:32:10.943416Z","iopub.status.idle":"2025-10-13T13:32:10.949164Z","shell.execute_reply.started":"2025-10-13T13:32:10.943392Z","shell.execute_reply":"2025-10-13T13:32:10.948397Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"Method 2:\")\nfor sales in tf_dataset:\n    print(sales.numpy())\n\n# this encoded object can be viewed with the help of numpy array\n# OR\nprint(\"method 2:\")\nfor sales in tf_dataset.as_numpy_iterator():\n    print(sales)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:35:16.701985Z","iopub.execute_input":"2025-10-13T13:35:16.702298Z","iopub.status.idle":"2025-10-13T13:35:16.714679Z","shell.execute_reply.started":"2025-10-13T13:35:16.702276Z","shell.execute_reply":"2025-10-13T13:35:16.713701Z"}},"outputs":[{"name":"stdout","text":"Method 2:\n21\n22\n-108\n31\n-1\n32\n34\n31\nmethod 2:\n21\n22\n-108\n31\n-1\n32\n34\n31\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"for sales in tf_dataset.take(3):\n    print(sales.numpy())\n\n# using take(), we can take how much data we want from the object\n# works like sample for dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:36:48.377407Z","iopub.execute_input":"2025-10-13T13:36:48.377698Z","iopub.status.idle":"2025-10-13T13:36:48.390164Z","shell.execute_reply.started":"2025-10-13T13:36:48.377680Z","shell.execute_reply":"2025-10-13T13:36:48.389306Z"}},"outputs":[{"name":"stdout","text":"21\n22\n-108\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Sales number can't be negetive, we need to filter the values \ntf_dataset = tf_dataset.filter(lambda x: x>0)\n\nfor sales in tf_dataset:\n    print(sales.numpy())\n\n# you can see the negetive samples are gone!!! booom","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:34.978638Z","iopub.execute_input":"2025-10-13T13:39:34.979002Z","iopub.status.idle":"2025-10-13T13:39:35.030257Z","shell.execute_reply.started":"2025-10-13T13:39:34.978976Z","shell.execute_reply":"2025-10-13T13:39:35.029118Z"}},"outputs":[{"name":"stdout","text":"21\n22\n31\n32\n34\n31\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# these numbers are in us dollars, to convert them to indian rupees \n# we can use map() there\n\ntf_dataset = tf_dataset.map(lambda x: x*72) # 1 dollars = 72 rupees\n\nfor sales in tf_dataset:\n    print(sales.numpy())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:41:20.262111Z","iopub.execute_input":"2025-10-13T13:41:20.262471Z","iopub.status.idle":"2025-10-13T13:41:20.310043Z","shell.execute_reply.started":"2025-10-13T13:41:20.262444Z","shell.execute_reply":"2025-10-13T13:41:20.309210Z"}},"outputs":[{"name":"stdout","text":"1512\n1584\n2232\n2304\n2448\n2232\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# if you want to randomly shuffle these elemnts for training purpose\n\ntf_dataset = tf_dataset.shuffle(2)\n\nfor sales in tf_dataset:\n    print(sales.numpy())\n\n# randomly re-arranged elemnts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:42:25.161573Z","iopub.execute_input":"2025-10-13T13:42:25.162015Z","iopub.status.idle":"2025-10-13T13:42:25.190760Z","shell.execute_reply.started":"2025-10-13T13:42:25.161991Z","shell.execute_reply":"2025-10-13T13:42:25.189995Z"}},"outputs":[{"name":"stdout","text":"1512\n2232\n2304\n1584\n2448\n2232\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# If you want to create batches of data\nprint(\"before batching:\")\nfor sales_batch in tf_dataset:\n    print(sales_batch.numpy())\n\n\nprint(\"after batching:\")\nfor sales_batch in tf_dataset.batch(2):\n    print(sales_batch.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:45:19.420478Z","iopub.execute_input":"2025-10-13T13:45:19.421375Z","iopub.status.idle":"2025-10-13T13:45:19.461168Z","shell.execute_reply.started":"2025-10-13T13:45:19.421337Z","shell.execute_reply":"2025-10-13T13:45:19.460357Z"}},"outputs":[{"name":"stdout","text":"before batching:\n1512\n1584\n2232\n2448\n2232\n2304\nafter batching:\n[1512 1584]\n[2304 2448]\n[2232 2232]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# these can be done in single line\n\ntf_dataset = tf.data.Dataset.from_tensor_slices(daily_sales_number)\n\ntf_dataset = tf_dataset.filter(lambda x: x>0).map(lambda y: y*72).shuffle(2).batch(2)\n\nfor sales in tf_dataset:\n    print(sales.numpy())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:50:37.974637Z","iopub.execute_input":"2025-10-13T13:50:37.975288Z","iopub.status.idle":"2025-10-13T13:50:38.042681Z","shell.execute_reply.started":"2025-10-13T13:50:37.975263Z","shell.execute_reply":"2025-10-13T13:50:38.041797Z"}},"outputs":[{"name":"stdout","text":"[1584 2232]\n[1512 2304]\n[2232 2448]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"images_ds = tf.data.Dataset.list_files(\"/images\", shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_count = len(images_ds)\nimage_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(images_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in images_ds.take(3):\n    print(file.numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_ds = images_ds.shuffle(200)\nfor file in images_ds.take(3):\n    print(file.numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = [\"cat\",\"dog\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(image_count*0.8)\ntrain_ds = images_ds.take(train_size)\ntest_ds = images_ds.skip(train_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlen(train_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_label(file_path):\n    import os\n    parts = tf.strings.split(file_path, os.path.sep)\n    return parts[-2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_label(\"images\\\\dog\\\\20 Reasons Why Cats Make the Best Pets....jpg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_image(file_path):\n    label = get_label(file_path)\n    img = tf.io.read_file(file_path) # load the raw data from the file as a string\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.resize(img, [128, 128])\n    return img, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, label = process_image(\"images\\\\cat\\\\20 Reasons Why Cats Make the Best Pets....jpg\")\nimg.numpy()[:2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = train_ds.map(process_image)\ntest_ds = test_ds.map(process_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image, label in train_ds.take(1):\n    print(\"****\",image)\n    print(\"****\",label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def scale(image, label):\n    return image/255, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = train_ds.map(scale)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for image, label in train_ds.take(5):\n    print(\"****Image: \",image.numpy()[0][0])\n    print(\"****Label: \",label.numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Explanation of Image Dataset Pipeline**\n\nThe second part of the code focuses on using the `tf.data` API to efficiently load and preprocess a large collection of images, which is essential for deep learning.\n\n### 1\\. Data Ingestion and Splitting\n\n| Code/Concept | Explanation | Rationale |\n| :--- | :--- | :--- |\n| `images_ds = tf.data.Dataset.list_files('images/*/*', shuffle=False)` | **Listing Files** | Creates a dataset where each element is a **file path string** (a TensorFlow `string` tensor). The `*/*` pattern searches for files within immediate subdirectories (e.g., `cat/` and `dog/`) inside the `images/` directory. | It avoids loading all image data into memory at once, consuming only the file paths. This is the **starting point** for efficient, scalable image loading. |\n| `images_ds = images_ds.shuffle(200)` | **Shuffling** | Randomly shuffles the order of the file paths. The argument `200` is the buffer size. | Ensures that batches drawn during training are **not sequential** (e.g., all \"cat\" images followed by all \"dog\" images), which is crucial for **preventing bias** and aiding generalization. |\n| `train_ds = images_ds.take(train_size)`<br>`test_ds = images_ds.skip(train_size)` | **Splitting** | Splits the shuffled file path dataset into training (80%) and testing (20%) datasets. | Standard practice in machine learning to evaluate the model's performance on **unseen data** (`test_ds`) after it has been trained on the majority of the data (`train_ds`). |\n\n\n\n### 2\\. Preprocessing Functions\n\nThese Python functions define the *transformations* that will be applied to every element (file path) in the dataset.\n\n| Function | Concept | Explanation |\n| :--- | :--- | :--- |\n| `get_label(file_path)` | **Extracting Label** | Splits the file path string based on the path separator (`os.path.sep`) and returns the **second-to-last part**. For a path like `images/dog/file.jpg`, this extracts the directory name: **`dog`**. | This leverages the common practice of organizing image datasets by putting images for each class into a directory named after that class. |\n| `process_image(file_path)` | **Image Loading & Decoding** | 1. Calls `get_label` to get the class name. 2. `tf.io.read_file`: Reads the raw bytes of the image file. 3. `tf.image.decode_jpeg`: Decodes the raw bytes into a full-color tensor. 4. `tf.image.resize`: Resizes the image to a fixed target size of $128 \\times 128$. | This is the heavy lifting: it converts a simple file path into a usable, consistent tensor format. Resizing is essential because deep learning models require **fixed-size inputs**. |\n\n\n\n### 3\\. Dataset Mapping and Transformation\n\nThe following steps apply the preprocessing functions to the entire dataset using the `map` function.\n\n| Code/Concept | Explanation | Rationale |\n| :--- | :--- | :--- |\n| `train_ds = train_ds.map(process_image)`<br>`test_ds = test_ds.map(process_image)` | **Mapping (Applying `process_image`)** | The `map` function applies the `process_image` function (which returns `(image_tensor, label_tensor)`) to every file path in the training and testing datasets. | This is an **efficient parallel operation**. TensorFlow handles the heavy I/O and CPU work (reading and resizing) in the background, preparing the data for the GPU/TPU training step. |\n| `def scale(image, label):`<br>`return image/255, label` | **Scaling Function** | Defines a function to normalize the pixel values from the standard $0-255$ range to the $0.0-1.0$ range. | **Normalization** is critical for model stability and convergence. Scaling inputs to a small, consistent range ensures that gradients are well-behaved during backpropagation. |\n| `train_ds = train_ds.map(scale)` | **Applying Scaling** | Applies the `scale` function to the output of the `process_image` function. The dataset now yields normalized image tensors and label strings. | Completes the standard preprocessing steps, making the image data ready for a neural network. |\n\n\n\n### 4\\. Final Preparation (Missing but Necessary)\n\nAlthough the code stops at the `scale` step, a full machine learning pipeline would require these final two steps:\n\n1.  **Label Encoding:** The labels (`b'cat'`, `b'dog'`) need to be converted to numerical one-hot vectors (e.g., $[1, 0]$ for cat, $[0, 1]$ for dog).\n2.  **Batching and Prefetching:** The dataset must be grouped into batches for model training, and prefetching should be enabled to overlap data loading with model computation.\n\nThis setup showcases a powerful, idiomatic way to handle large datasets in TensorFlow by leveraging the **lazy evaluation** and **parallelism** of the `tf.data.Dataset` API.","metadata":{}}]}