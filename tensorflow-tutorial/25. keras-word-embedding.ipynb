{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.578196Z","iopub.execute_input":"2025-10-12T04:56:31.579125Z","iopub.status.idle":"2025-10-12T04:56:31.586034Z","shell.execute_reply.started":"2025-10-12T04:56:31.579084Z","shell.execute_reply":"2025-10-12T04:56:31.584453Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"## **Embdedding layer**\n- tokenize\n- encode\n- padding\n- embedding\n- weights","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\nfrom tensorflow.keras.preprocessing.text import one_hot \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # Corrected typo: pad_sequence -> pad_sequences\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.587973Z","iopub.execute_input":"2025-10-12T04:56:31.588249Z","iopub.status.idle":"2025-10-12T04:56:31.603556Z","shell.execute_reply.started":"2025-10-12T04:56:31.588227Z","shell.execute_reply":"2025-10-12T04:56:31.602387Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.604684Z","iopub.execute_input":"2025-10-12T04:56:31.605001Z","iopub.status.idle":"2025-10-12T04:56:31.622922Z","shell.execute_reply.started":"2025-10-12T04:56:31.604979Z","shell.execute_reply":"2025-10-12T04:56:31.621642Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"reviews = ['nice food',\n        'amazing restaurant',\n        'too good',\n        'just loved it!',\n        'will go again',\n        'horrible food',\n        'never go there',\n        'poor service',\n        'poor quality',\n        'needs improvement']\n\nsentiment = np.array([1,1,1,1,1,0,0,0,0,0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.624169Z","iopub.execute_input":"2025-10-12T04:56:31.624505Z","iopub.status.idle":"2025-10-12T04:56:31.642204Z","shell.execute_reply.started":"2025-10-12T04:56:31.624476Z","shell.execute_reply":"2025-10-12T04:56:31.641166Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# gives unique number to the words to embed \none_hot('ncie food', 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.644389Z","iopub.execute_input":"2025-10-12T04:56:31.644686Z","iopub.status.idle":"2025-10-12T04:56:31.668230Z","shell.execute_reply.started":"2025-10-12T04:56:31.644652Z","shell.execute_reply":"2025-10-12T04:56:31.666970Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"[2, 29]"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"vocab_size = 30\nencoded_reviews = [one_hot(d, vocab_size) for d in reviews]\nprint(encoded_reviews)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.669829Z","iopub.execute_input":"2025-10-12T04:56:31.670169Z","iopub.status.idle":"2025-10-12T04:56:31.685669Z","shell.execute_reply.started":"2025-10-12T04:56:31.670135Z","shell.execute_reply":"2025-10-12T04:56:31.684626Z"}},"outputs":[{"name":"stdout","text":"[[18, 29], [26, 20], [14, 8], [26, 26, 24], [29, 6, 6], [2, 29], [13, 6, 7], [20, 23], [20, 4], [1, 20]]\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# TO have equal length of all the reviews used here\n\nmax_length = 4\npadded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\nprint(padded_reviews)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.686745Z","iopub.execute_input":"2025-10-12T04:56:31.687628Z","iopub.status.idle":"2025-10-12T04:56:31.706601Z","shell.execute_reply.started":"2025-10-12T04:56:31.687592Z","shell.execute_reply":"2025-10-12T04:56:31.705781Z"}},"outputs":[{"name":"stdout","text":"[[18 29  0  0]\n [26 20  0  0]\n [14  8  0  0]\n [26 26 24  0]\n [29  6  6  0]\n [ 2 29  0  0]\n [13  6  7  0]\n [20 23  0  0]\n [20  4  0  0]\n [ 1 20  0  0]]\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Now do the embedding to the reviews\n\nembeded_vector_size = 5\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.707760Z","iopub.execute_input":"2025-10-12T04:56:31.708054Z","iopub.status.idle":"2025-10-12T04:56:31.727160Z","shell.execute_reply.started":"2025-10-12T04:56:31.708029Z","shell.execute_reply":"2025-10-12T04:56:31.725901Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"X = padded_reviews\ny = sentiment","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.728276Z","iopub.execute_input":"2025-10-12T04:56:31.728552Z","iopub.status.idle":"2025-10-12T04:56:31.746374Z","shell.execute_reply.started":"2025-10-12T04:56:31.728530Z","shell.execute_reply":"2025-10-12T04:56:31.745262Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:59:16.363924Z","iopub.execute_input":"2025-10-12T04:59:16.364214Z","iopub.status.idle":"2025-10-12T04:59:16.394886Z","shell.execute_reply.started":"2025-10-12T04:59:16.364194Z","shell.execute_reply":"2025-10-12T04:59:16.393768Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m5\u001b[0m)           │           \u001b[38;5;34m150\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171\u001b[0m (684.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171</span> (684.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171\u001b[0m (684.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171</span> (684.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"model.fit(X, y, epochs=50, verbose=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:31.787117Z","iopub.execute_input":"2025-10-12T04:56:31.788156Z","iopub.status.idle":"2025-10-12T04:56:35.609162Z","shell.execute_reply.started":"2025-10-12T04:56:31.788128Z","shell.execute_reply":"2025-10-12T04:56:35.608114Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ad32c3d3850>"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(X, y)\naccuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:56:35.610147Z","iopub.execute_input":"2025-10-12T04:56:35.610423Z","iopub.status.idle":"2025-10-12T04:56:35.933883Z","shell.execute_reply.started":"2025-10-12T04:56:35.610401Z","shell.execute_reply":"2025-10-12T04:56:35.932620Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 1.0000 - loss: 0.6248\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"weights = model.get_layer('embedding').get_weights()[0]\nlen(weights)\n\n# Same as vocab size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:58:09.134508Z","iopub.execute_input":"2025-10-12T04:58:09.134927Z","iopub.status.idle":"2025-10-12T04:58:09.143558Z","shell.execute_reply.started":"2025-10-12T04:58:09.134898Z","shell.execute_reply":"2025-10-12T04:58:09.142477Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"weights[18] # for review \"nice\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T04:58:58.315890Z","iopub.execute_input":"2025-10-12T04:58:58.316785Z","iopub.status.idle":"2025-10-12T04:58:58.323213Z","shell.execute_reply.started":"2025-10-12T04:58:58.316708Z","shell.execute_reply":"2025-10-12T04:58:58.322171Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"array([ 0.04153833, -0.07388881, -0.09074349,  0.08594728,  0.00381914],\n      dtype=float32)"},"metadata":{}}],"execution_count":66},{"cell_type":"markdown","source":"## 1. Problem Setup: Text Classification\n\nGoal:\nTrain a simple neural network to classify short text reviews as **positive (1)** or **negative (0)**.\n\nExample inputs:\n\n```\n“nice food” → positive  \n“poor service” → negative\n```\n\nThe model learns to associate patterns of words (or combinations) with sentiment.\n\n\n\n## 2. Text to Numeric Conversion — One-Hot Encoding\n\nBefore a model can process text, it must be converted into numbers.\n\n```python\nfrom tensorflow.keras.preprocessing.text import one_hot\nvocab_size = 30\nencoded_reviews = [one_hot(d, vocab_size) for d in reviews]\n```\n\n* **One-hot encoding here doesn’t mean a long binary vector**.\n  Keras’s `one_hot()` function maps each word to a **unique integer index** between `1` and `vocab_size`.\n\nExample:\n\n```\n“amazing restaurant” → [4, 23]\n```\n\nSo the vocabulary of all possible words is treated as numbers from 1 to 30.\n\n**Conceptually:**\n\n* Each word is just a placeholder ID.\n* The model has no semantic understanding yet — it will learn that later through embeddings.\n\n\n\n## 3. Sequence Padding\n\nNeural networks require all input sequences to have **equal length**.\n\n```python\npadded_reviews = pad_sequences(encoded_reviews, maxlen=4, padding='post')\n```\n\n* Pads shorter reviews with zeros (`0` represents “no word”).\n* Ensures each input sequence has length = 4.\n\nExample:\n\n```\n[13, 21] → [13, 21, 0, 0]\n[8, 15, 16] → [8, 15, 16, 0]\n```\n\n**Conceptually:**\nPadding allows batch processing and fixed-size input layers.\n\n\n## 4. Embedding Layer — Core Concept\n\n```python\nmodel.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length))\n```\n\nThis is the most important conceptual part.\n\n### What it does:\n\n* Takes integer word IDs (like `[13, 21, 0, 0]`)\n* Converts each into a **dense vector** of length 5 (the `embeded_vector_size`)\n* Learns these vectors during training\n\nSo, the output shape becomes `(batch_size, max_length, embed_dim)`\n→ in this case, `(None, 4, 5)`\n\n### Conceptually:\n\n* Each word gets represented as a **learned feature vector** (embedding).\n* The embedding captures meaning — similar words end up with similar vectors.\n* This is the **same concept as Word2Vec**, but learned **within the task** instead of pre-trained.\n\n\n## 5. Flatten and Dense Layers\n\n```python\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n```\n\n* **Flatten:** Converts the 2D embedding matrix (4 × 5 = 20 values) into a single vector `[20]`.\n* **Dense layer:** Applies a sigmoid classifier to produce an output between 0 and 1.\n\n### Conceptually:\n\n* The flattened embeddings represent the sentence as a feature vector.\n* The dense neuron learns to map that representation → sentiment label.\n\n\n\n## 6. Compilation and Training\n\n```python\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X, y, epochs=50, verbose=0)\n```\n\n* **Loss:** Binary cross-entropy (since it’s a 2-class problem)\n* **Optimizer:** Adam — adjusts weights to minimize loss\n* **Training:** Each epoch updates both:\n\n  * The embedding weights (word representations)\n  * The classifier weights\n\n\n\n## 7. Model Evaluation\n\n```python\nloss, accuracy = model.evaluate(X, y)\n```\n\nAccuracy = 1.0 means it perfectly classified the 10 sample reviews (tiny dataset).\n\n**Conceptually:**\nThe model learned meaningful relationships between words and sentiment from just a few examples — but this accuracy is not generalizable (overfitting on small data).\n\n\n\n## 8. Inspecting the Learned Word Embeddings\n\n```python\nweights = model.get_layer('embedding').get_weights()[0]\n```\n\n* `weights` is a matrix of shape `(vocab_size, embed_dim)` → `(30, 5)`\n* Each row is the **embedding vector** for one word index.\n\nExample:\n\n```\nweights[13] → embedding for the word with ID 13 (“nice”)\nweights[4]  → embedding for ID 4 (“amazing”)\n```\n\n### Conceptually:\n\nThese vectors encode **learned meaning**.\nWords used in similar contexts will have **similar embedding vectors**.\nThat’s how deep learning captures semantic similarity.\n\n\n\n## 9. What the Model Really Learned\n\n* The **Embedding layer** learned to map words into a 5-dimensional “meaning space”.\n* The **Dense layer** learned to classify the sentence-level representation into 0 or 1.\n* This is a **task-specific embedding** — the meaning is aligned with “positive” or “negative” sentiment.\n\n---\n\n## 10. Concept Summary\n\n| Step              | Concept                       | Role                                |\n| ----------------- | ----------------------------- | ----------------------------------- |\n| One-hot           | Integer IDs for words         | Converts text → numeric tokens      |\n| Padding           | Equalizes sequence lengths    | Enables batch training              |\n| Embedding         | Dense, trainable word vectors | Learns semantic meaning             |\n| Flatten           | Merges all embeddings         | Forms a sentence vector             |\n| Dense + Sigmoid   | Binary classifier             | Predicts sentiment                  |\n| Embedding weights | Learned word meaning          | Shows task-based word relationships |\n","metadata":{}}]}
