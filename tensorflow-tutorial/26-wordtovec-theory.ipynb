{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. What is a Feature Vector in Word2Vec?\n\nIn **Word2Vec**, every **word** in your vocabulary is represented by a **dense numerical vector** — called a **feature vector** or **word embedding**.\n\nExample:\n\n| Word    | Feature Vector (size = 5)        |\n| ------- | -------------------------------- |\n| “king”  | [0.21, 0.58, -0.31, 0.44, 0.12]  |\n| “queen” | [0.19, 0.63, -0.27, 0.51, 0.08]  |\n| “apple” | [-0.11, 0.44, 0.75, -0.33, 0.19] |\n\nEach word becomes a **point in a high-dimensional space** (often 100–300 dimensions in real models).\nThe idea is that **semantic relationships** between words are captured as **distances and directions** in this space.\n\n\n\n## 2. What These Vectors Represent\n\nEach **dimension** in a Word2Vec vector doesn’t have a human-readable meaning like “gender” or “royalty.”\nBut together, the **pattern across dimensions** captures:\n\n* Contextual similarity (words appearing in similar contexts have similar vectors)\n* Relationships like:\n\n  * **king − man + woman ≈ queen**\n  * **Paris − France + Italy ≈ Rome**\n\nSo, these feature vectors encode both **similarity** and **relationships**.\n\n\n\n## 3. How Word2Vec Learns These Vectors\n\nWord2Vec is **not just a lookup**; it learns these feature vectors using a shallow neural network trained on large text corpora.\n\nThere are two main architectures:\n\n### (a) CBOW (Continuous Bag of Words)\n\n* Predicts the **target word** from the **context words**.\n  Example: from “The cat _ on the mat”, predict “sat”.\n\n### (b) Skip-Gram\n\n* Does the opposite: predicts **context words** from the **target word**.\n  Example: from “cat”, predict words like “the”, “sat”, “on”.\n\nDuring training:\n\n* Each word starts as a random vector.\n* The model adjusts these vectors so words that appear in similar contexts end up close in vector space.\n\n\n\n## 4. The Vector Space Concept\n\nImagine a geometric space where:\n\n* Similar words cluster together.\n  (“good”, “nice”, “great” → nearby)\n* Opposite or unrelated words are far apart.\n  (“good” vs “terrible”)\n* Semantic axes naturally form:\n  Gender, tense, country–capital, singular–plural, etc.\n\nExample visualization (2D simplification):\n\n```\n        woman      king\n           \\       /\n            \\     /\n             \\   /\n              \\ /\n              man\n```\n\nIn higher dimensions, these relationships are encoded as **vector directions and magnitudes**.\n\n\n\n## 5. Why It’s Called a “Feature” Vector\n\nEach number in the vector represents a **latent feature** the model has learned automatically — not predefined by humans.\n\nFor example:\n\n* Some dimensions may loosely encode sentiment (positive ↔ negative)\n* Some may encode part of speech (noun ↔ verb)\n* Others may encode syntactic or semantic patterns\n\nThese are **emergent features** — the model discovers them purely from word co-occurrence patterns.\n\n\n\n## 6. Example: Training Outcome (Simplified)\n\nLet’s say the vocabulary is `[king, queen, man, woman]`, embedding size = 3\n\n| Word  | Feature Vector     |\n| ----- | ------------------ |\n| king  | [0.7, 0.2, 0.9]    |\n| queen | [0.69, 0.19, 0.91] |\n| man   | [0.5, 0.1, 0.4]    |\n| woman | [0.49, 0.09, 0.42] |\n\nHere:\n\n* “king” and “queen” are close because they share similar contexts (“throne”, “crown”, “royal”)\n* “man” and “woman” are close because they appear in similar contexts (“person”, “adult”)\n\nAnd the **difference vector** between king–man and queen–woman is nearly the same direction — showing learned relational structure.\n\n\n\n## 7. How to Use These Feature Vectors\n\nOnce trained, these embeddings can be used for:\n\n* **Similarity:**\n  `cosine_similarity(vec(\"good\"), vec(\"great\"))`\n* **Analogy solving:**\n  `vec(\"king\") - vec(\"man\") + vec(\"woman\") ≈ vec(\"queen\")`\n* **Downstream tasks:**\n  Feed embeddings into classifiers, LSTMs, or transformers.\n\n\n\n## 8. In Short\n\n| Concept            | Meaning                                              |\n| ------------------ | ---------------------------------------------------- |\n| **Feature vector** | A numeric representation of a word capturing meaning |\n| **Dimensions**     | Latent semantic features learned from data           |\n| **Similarity**     | Geometric closeness = contextual similarity          |\n| **Learned from**   | Word co-occurrence patterns in text                  |\n| **Used for**       | Semantic tasks, NLP pipelines, and transfer learning |","metadata":{}},{"cell_type":"markdown","source":"## **Continuous Bag of Words (CBOW)**\n\n### 1. Introduction\nThe **Continuous Bag of Words (CBOW)** model is one of the two main architectures used in **Word2Vec** (the other being Skip-Gram).  \nIt aims to predict a **target word** based on its **surrounding context words**.  \n\nCBOW helps the network learn vector representations of words — called **embeddings** — that capture semantic relationships.\n\n\n\n### 2. Intuitive Example\n\nConsider the sentence:  \n> \"The cat sat on the mat\"\n\nIf the context window size is 2, and the target word is **\"sat\"**,  \nthen the **context words** are:  \n`[\"The\", \"cat\", \"on\", \"the\"]`\n\nThe CBOW task:  \n> Predict \"sat\" from the context [\"The\", \"cat\", \"on\", \"the\"].\n\n\n\n### 3. Model Architecture\n\nCBOW is a **shallow neural network** with three layers:\n1. **Input layer** – represents context words as one-hot encoded vectors.\n2. **Hidden layer** – shared weight matrix that acts as the word embedding space.\n3. **Output layer** – predicts the target word using a softmax function.\n\nThe overall flow:\n","metadata":{}},{"cell_type":"markdown","source":"\n\n\n### 4. Working Mechanism\n\n#### Step 1: Input Representation\nEach context word is converted into a **one-hot vector**.  \nFor a vocabulary of size `V`, the vector has a single `1` at the index of the word, and `0`s elsewhere.\n\nExample:  \nIf `V = 10` and the word “cat” is the 3rd in the vocabulary:\n","metadata":{}},{"cell_type":"markdown","source":"\n#### Step 2: Hidden Layer\nEach one-hot input vector is multiplied by a shared weight matrix **W** of size `(V × N)`  \n(where `N` is the embedding dimension).\n\nThis produces an **embedding** for each context word.\n\nCBOW **averages the embeddings** of all context words to get a single context vector `h`.\n\n#### Step 3: Output Layer\nThe context vector `h` is multiplied by another matrix `W'` and passed through a **softmax** layer to produce probabilities for all words in the vocabulary.\n\nThe word with the highest probability is the **predicted target word**.\n\n\n\n### 5. Mathematical Formulation\n\nGiven:\n- Vocabulary size: `V`\n- Embedding dimension: `N`\n- Context window size: `n`\n- Target word: `w_t`\n- Context words: `w_{t-n}, ..., w_{t-1}, w_{t+1}, ..., w_{t+n}`\n\n**Input:**\n\\[\n\\text{Context words} = \\{ w_{t-n}, ..., w_{t+n} \\}\n\\]\n\n**Hidden layer (average context vector):**\n\\[\nh = \\frac{1}{2n} \\sum_{-n \\le j \\le n, j \\ne 0} W^T x_{t+j}\n\\]\n\n**Output layer (softmax prediction):**\n\\[\ny = \\text{softmax}(W'^T h)\n\\]\n\n**Loss function (cross-entropy):**\n\\[\nE = -\\log P(w_t | \\text{context})\n\\]\n\n\n\n### 6. Why “Continuous Bag of Words”?\n\n- **Continuous** – works with continuous-valued word embeddings.\n- **Bag of Words** – the order of context words is ignored (like a bag of words).\n\n\n\n### 7. Strengths and Weaknesses\n\n| Aspect | Description |\n|--------|--------------|\n| **Strengths** | Simple, efficient for large datasets, fast to train |\n| **Weaknesses** | Ignores word order, less effective for rare words |\n| **Best used for** | Capturing meaning from frequent co-occurrences |\n\n\n\n### 8. Comparison with Skip-Gram\n\n| Feature | CBOW | Skip-Gram |\n|----------|------|-----------|\n| Predicts | Target word from context | Context words from target |\n| Training speed | Faster | Slower |\n| Works better for | Frequent words | Rare words |\n| Input | Multiple context words | Single target word |\n| Output | One target word | Multiple context words |\n\n\n\n### 9. Summary\n\nCBOW helps a model learn **word embeddings** by predicting a missing word from its surrounding context.  \nThese embeddings capture **semantic similarity**, meaning similar words (like “king” and “queen”) end up with vectors close to each other in the embedding space.\n\nIn practice, CBOW embeddings are used as a foundational feature in many NLP models, including language models, text classifiers, and neural translation systems.\n","metadata":{}},{"cell_type":"markdown","source":"## Skip-Gram Model\n\n### 1. Introduction\nThe **Skip-Gram model** is one of the two main architectures used in **Word2Vec** (the other is CBOW).  \nIt aims to do the opposite of CBOW — instead of predicting a word from its context, Skip-Gram predicts the **context words** given a **target word**.\n\nThis approach helps capture how words are used in different contexts and is especially powerful for learning good representations of **rare words**.\n\n\n\n### 2. Intuitive Example\n\nSentence:\n> \"The cat sat on the mat\"\n\nIf the window size is 2, and the target word is **“sat”**,  \nthen the **context words** are:\n`[\"The\", \"cat\", \"on\", \"the\"]`\n\nSkip-Gram’s task:\n> Given the target “sat”, predict the surrounding words [“The”, “cat”, “on”, “the”].\n\n\n\n### 3. Model Architecture\n\nSkip-Gram is a **simple neural network** with three layers:\n1. **Input layer** – one-hot encoded target word  \n2. **Hidden layer** – weight matrix that acts as the embedding lookup  \n3. **Output layer** – predicts probabilities for all words in the vocabulary using a softmax function\n\nThe structure:\n","metadata":{}},{"cell_type":"markdown","source":"\n---\n\n### 4. Working Mechanism\n\n#### Step 1: Input Representation\nThe input word (target word) is represented as a **one-hot vector** of length `V` (the vocabulary size).  \nExample:\n","metadata":{}},{"cell_type":"markdown","source":"\n#### Step 2: Hidden Layer (Embedding Lookup)\nThe one-hot vector is multiplied by a weight matrix **W** of size `(V × N)`,  \nwhere `N` is the embedding dimension.\n\nThis effectively selects the **embedding vector** corresponding to the target word.\n\n#### Step 3: Output Layer\nThe resulting embedding is multiplied by another matrix **W'** of size `(N × V)` and passed through a **softmax** to predict the probabilities of each word in the vocabulary being a context word.\n\n#### Step 4: Training Objective\nThe model is trained to **maximize the probability of actual context words** while minimizing the probability of unrelated words.\n\n\n\n### 5. Mathematical Formulation\n\nGiven:\n- Vocabulary size: `V`\n- Embedding dimension: `N`\n- Context window size: `n`\n- Target word: `w_t`\n- Context words: `w_{t-n}, ..., w_{t-1}, w_{t+1}, ..., w_{t+n}`\n\nThe Skip-Gram objective is to maximize the log probability:\n\n\\[\nJ = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-n \\le j \\le n, j \\ne 0} \\log P(w_{t+j} | w_t)\n\\]\n\nWhere:\n\n\\[\nP(w_{t+j} | w_t) = \\frac{\\exp(v_{w_{t+j}}'^{T} v_{w_t})}{\\sum_{w=1}^{V} \\exp(v_w'^{T} v_{w_t})}\n\\]\n\nHere:\n- \\( v_{w_t} \\) → vector representation of the target word  \n- \\( v_{w_{t+j}}' \\) → vector representation of a context word  \n- The denominator normalizes probabilities across the vocabulary.\n\n\n\n### 6. Why Use Skip-Gram?\n\n| Feature | Description |\n|----------|--------------|\n| **Direction** | Predicts context from target |\n| **Performance** | Works well with small datasets |\n| **Handling rare words** | Learns better representations for infrequent words |\n| **Flexibility** | Captures asymmetric relationships (e.g., “doctor” → “hospital”, but not vice versa) |\n\n\n\n### 7. Skip-Gram vs CBOW\n\n| Feature | Skip-Gram | CBOW |\n|----------|------------|------|\n| **Predicts** | Context words from target word | Target word from context |\n| **Training speed** | Slower | Faster |\n| **Works better for** | Rare words | Frequent words |\n| **Direction** | One → Many | Many → One |\n| **Context handling** | Treats each context pair independently | Averages all context embeddings |\n\n\n\n### 8. Improving Efficiency: Negative Sampling and Hierarchical Softmax\n\nSince computing the full softmax for every word in a large vocabulary is expensive, Skip-Gram often uses:\n1. **Negative Sampling** – updates weights for only a few negative examples per step, instead of all words.  \n2. **Hierarchical Softmax** – organizes the vocabulary as a binary tree to speed up probability computation.\n\nThese techniques make Skip-Gram scalable to massive corpora.\n\n\n\n### 9. Summary\n\nThe Skip-Gram model learns embeddings by predicting the words that appear around a given target word.  \nIt captures rich semantic and syntactic relationships — words that appear in similar contexts end up close together in the embedding space.\n\nFor example:\n- “king” and “queen” → similar vectors  \n- “Paris” and “France” → similar relationship as “Tokyo” and “Japan”\n\nThese learned embeddings are widely used in NLP tasks such as:\n- Sentiment analysis  \n- Named entity recognition  \n- Machine translation  \n- Text classification\n","metadata":{}}]}