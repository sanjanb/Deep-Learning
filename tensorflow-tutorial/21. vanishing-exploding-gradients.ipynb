{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vanishing Gradients in RNNs\n\n\n## 1. Introduction\n\n**Vanishing gradients** is a common problem in training deep neural networks and RNNs, especially with long sequences.\n\n* During **backpropagation**, gradients are propagated backward through layers or time steps.\n* If gradients become **very small**, the network **fails to learn long-term dependencies**.\n* This is particularly a problem in **RNNs** because hidden states are repeatedly multiplied by weights at each time step.\n\n\n\n## 2. How It Happens\n\nConsider an RNN with hidden state update:\n\n[\nh_t = f(W_{hh} h_{t-1} + W_{xh} x_t)\n]\n\nDuring backpropagation, the gradient of the loss w.r.t an earlier hidden state:\n\n[\n\\frac{\\partial L}{\\partial h_{t-k}} = \\frac{\\partial L}{\\partial h_t} \\cdot \\prod_{i=t-k+1}^{t} f'(h_i) W_{hh}\n]\n\n* (f') is the derivative of the activation function (e.g., tanh, sigmoid)\n* (W_{hh}) is the recurrent weight matrix\n\n**Problem:**\n\n* For **sigmoid or tanh**, (f'(h) < 1)\n* Multiplying many small numbers → gradient approaches **zero**\n\n[\n\\prod f'(h_i) W_{hh} \\approx 0\n]\n\n* Early layers or time steps **learn very slowly**, losing long-term memory.\n\n\n\n## 3. Consequences in RNNs\n\n* Network **cannot capture long-range dependencies** in sequences\n* Only learns **short-term patterns**\n* Training may **stall**, resulting in poor performance for tasks like language translation or time series prediction with long sequences\n\n\n\n## 4. Illustration\n\n```\nSequence: x1 → x2 → x3 → x4 → x5\nHidden states: h1 → h2 → h3 → h4 → h5\n\nLoss depends on h5\n\nGradient w.r.t h1: \n∂L/∂h1 = ∂L/∂h5 * W^4 * f'(h1)f'(h2)...f'(h5)\n\nIf W < 1 and f' < 1 → ∂L/∂h1 ≈ 0\n```\n\n* Early time steps “forget” their influence.\n\n\n\n## 5. Solutions\n\n1. **Use LSTM or GRU**\n\n   * Special gates and cell state help maintain long-term memory\n   * Reduce vanishing gradient problem\n\n2. **Proper weight initialization**\n\n   * Initialize weights carefully to avoid very small values\n\n3. **Use ReLU activation**\n\n   * Instead of tanh/sigmoid, can help mitigate gradient shrinkage\n\n4. **Gradient clipping**\n\n   * Clip gradients to a minimum threshold to prevent them from vanishing (or exploding)\n\n5. **Shorter sequences / truncated BPTT**\n\n   * Backpropagate through smaller chunks of sequences\n\n---\n\n## 6. Summary Table\n\n| Concept            | Description                                                              |\n| ------------------ | ------------------------------------------------------------------------ |\n| Vanishing Gradient | Gradient becomes extremely small → early layers/time steps stop learning |\n| Cause              | Multiplication of small numbers during backpropagation through time      |\n| Effect             | RNN cannot capture long-term dependencies                                |\n| Solution           | LSTM/GRU, ReLU, weight initialization, gradient clipping, truncated BPTT |\n","metadata":{}},{"cell_type":"markdown","source":"# Exploding Gradients in RNNs\n\n\n## 1. Introduction\n\n**Exploding gradients** is the opposite problem of vanishing gradients.\n\n* During **backpropagation**, gradients can grow **very large**.\n* This can **destabilize training**, causing the model to produce NaNs or fail to converge.\n* Common in **deep networks** and **RNNs**, especially with long sequences.\n\n\n\n## 2. How It Happens\n\nConsider an RNN hidden state:\n\n[\nh_t = f(W_{hh} h_{t-1} + W_{xh} x_t)\n]\n\nDuring backpropagation:\n\n[\n\\frac{\\partial L}{\\partial h_{t-k}} = \\frac{\\partial L}{\\partial h_t} \\cdot \\prod_{i=t-k+1}^{t} f'(h_i) W_{hh}\n]\n\n* If (W_{hh}) has **eigenvalues > 1** or derivatives (f'(h_i) > 1)\n* Multiplying many large numbers → gradient **grows exponentially**\n\n[\n\\prod f'(h_i) W_{hh} \\gg 1\n]\n\n* Early layers or time steps **receive extremely large gradient updates**\n\n\n\n## 3. Consequences\n\n* Network weights can **explode to infinity**\n* Training becomes **unstable**\n* Loss function may **diverge**\n* Gradients may become **NaN**, preventing learning\n\n---\n\n## 4. Illustration\n\n```\nSequence: x1 → x2 → x3 → x4 → x5\nHidden states: h1 → h2 → h3 → h4 → h5\n\nGradient w.r.t h1: \n∂L/∂h1 = ∂L/∂h5 * W^4 * f'(h1)f'(h2)...f'(h5)\n\nIf W > 1 and f' > 1 → ∂L/∂h1 → very large\n```\n\n* Early time steps dominate updates → instability\n\n\n\n## 5. Solutions\n\n1. **Gradient Clipping**\n\n   * Limit gradients to a maximum threshold:\n\n   ```python\n   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n   ```\n\n   * Prevents excessively large updates.\n\n2. **Proper Weight Initialization**\n\n   * Avoid large initial weights for recurrent connections.\n\n3. **Use LSTM / GRU**\n\n   * Gates help stabilize gradient flow.\n\n4. **Use Appropriate Activation Functions**\n\n   * Avoid unbounded activations (like plain `tanh` with very large inputs).\n\n\n\n## 6. Comparison with Vanishing Gradients\n\n| Property           | Vanishing Gradient                    | Exploding Gradient                       |\n| ------------------ | ------------------------------------- | ---------------------------------------- |\n| Gradient Magnitude | Very small → 0                        | Very large → ∞                           |\n| Effect on Training | Early layers stop learning            | Training becomes unstable                |\n| Common Cause       | Small weights, sigmoids, deep network | Large weights, repeated multiplications  |\n| Solution           | LSTM/GRU, ReLU, truncated BPTT        | Gradient clipping, weight init, LSTM/GRU |\n\n---\n\n## 7. Key Takeaway\n\n* **Vanishing gradient:** prevents learning **long-term dependencies**\n* **Exploding gradient:** prevents **stable training**\n* Both are inherent challenges of deep RNNs.\n* Modern RNN architectures (LSTM, GRU) and techniques (gradient clipping) mitigate these issues.","metadata":{}}]}
