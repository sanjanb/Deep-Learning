{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Word Embeddings**\n\n### 1. **Introduction**\n\nWord embeddings are **dense vector representations** of words that capture their **semantic meaning**.\nUnlike one-hot vectors (which are sparse and high-dimensional), embeddings place similar words **closer together in a continuous vector space**.\n\nExample:\n[\n\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\n]\n\nThis shows that embeddings capture **relationships and analogies** between words.\n\n\n\n### 2. **Motivation**\n\nTraditional NLP models represent words using:\n\n* **One-hot encoding:** each word = one dimension in a large vector (e.g., 10,000 words → 10,000-d vector).\n* Problems:\n\n  * No notion of similarity (all words equally distant)\n  * Huge memory requirements\n  * Inefficient learning\n\n**Word embeddings** solve this by learning **lower-dimensional, continuous-valued vectors** that capture **context and meaning**.\n\n\n\n### 3. **Core Idea**\n\nEach word is represented as a vector in ( \\mathbb{R}^n ), typically where ( n ) = 100 to 300.\nWords that appear in **similar contexts** (e.g., \"doctor\", \"nurse\") end up with **similar vectors**.\n\nEmbedding spaces are learned **from data** — using models like **Word2Vec**, **GloVe**, or within neural networks (via **Embedding layers**).\n\n\n\n### 4. **Mathematical View**\n\nGiven a large corpus, embeddings are trained so that **words with similar contexts have similar vectors**.\n\nFormally, embeddings maximize:\n[\nP(\\text{context} | \\text{word}) \\text{ or } P(\\text{word} | \\text{context})\n]\ndepending on the training model.\n\n\n\n### 5. **Popular Word Embedding Models**\n\n#### a. **Word2Vec** (Mikolov et al., 2013)\n\nTwo main architectures:\n\n1. **CBOW (Continuous Bag of Words):**\n\n   * Predicts the current word based on its surrounding context.\n   * Faster for frequent words.\n     [\n     P(w_t | w_{t-m}, ..., w_{t+m})\n     ]\n\n2. **Skip-Gram:**\n\n   * Predicts the context words given a target word.\n   * Works better for rare words.\n     [\n     P(w_{t-m}, ..., w_{t+m} | w_t)\n     ]\n\n#### b. **GloVe (Global Vectors)**\n\n* Learns embeddings based on **global word co-occurrence statistics**.\n* Builds a matrix of how often words co-occur and factors it to generate embeddings.\n\n[\n\\text{word vector similarity} \\propto \\log(\\text{co-occurrence frequency})\n]\n\n#### c. **FastText**\n\n* Extends Word2Vec by representing words as **n-grams of characters**.\n* Captures **morphological features**, so it works better for rare and misspelled words.\n* Example: “playing” → “play”, “lay”, “ing”.\n\n\n\n### 6. **Word Embeddings in Deep Learning Models**\n\nIn neural NLP models, embeddings are learned **as part of the training** using an **Embedding layer**.\n\nExample in PyTorch:\n\n```python\nimport torch\nimport torch.nn as nn\n\nembedding = nn.Embedding(num_embeddings=10000, embedding_dim=300)\ninput_word_ids = torch.LongTensor([1, 5, 8, 12])\noutput = embedding(input_word_ids)\n\nprint(output.shape)  # torch.Size([4, 300])\n```\n\nHere:\n\n* `num_embeddings` = vocabulary size\n* `embedding_dim` = vector dimension\n* The layer maps word IDs → learned 300-d vectors\n\n---\n\n### 7. **Visualizing Embeddings**\n\nYou can project embeddings into 2D using PCA or t-SNE to observe clusters:\n\n* “king”, “queen”, “prince”, “princess” will cluster together.\n* “car”, “bus”, “train” form another cluster.\n\nExample:\n\n```python\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ntsne = TSNE(n_components=2)\nreduced = tsne.fit_transform(embedding.weight.detach().numpy())\n\nplt.scatter(reduced[:, 0], reduced[:, 1])\nplt.title(\"Word Embedding Space\")\nplt.show()\n```\n\n---\n\n### 8. **Applications**\n\n* Text classification\n* Sentiment analysis\n* Machine translation\n* Information retrieval\n* Named Entity Recognition (NER)\n* Transfer learning (pretrained embeddings)\n\n\n\n### 9. **Pre-trained Embeddings**\n\nYou can use pre-trained vectors instead of training from scratch:\n\n* **Word2Vec** (Google News, 3M words)\n* **GloVe** (Wikipedia + Gigaword)\n* **FastText** (Common Crawl, 600B tokens)\n\nUsing pre-trained embeddings helps models generalize faster with less data.\n\n\n\n### 10. **Key Takeaways**\n\n* Embeddings capture **semantic relationships** between words.\n* Represent words as **dense, low-dimensional vectors**.\n* Similar words → similar vectors.\n* Trained via **Word2Vec, GloVe, or neural networks**.\n* Core foundation for all modern NLP models.","metadata":{}},{"cell_type":"markdown","source":"# **Contextual Word Embeddings**\n\n### 1. **Why We Need Contextual Embeddings**\n\nTraditional embeddings such as **Word2Vec**, **GloVe**, and **FastText** represent each word with **a single, fixed vector**, regardless of context.\n\nExample:\n\n* Sentence 1: *“He sat by the **bank** of the river.”*\n* Sentence 2: *“She went to the **bank** to deposit money.”*\n\nIn static embeddings, both “bank” words would have **the same vector**, even though their meanings differ.\n\n**Contextual embeddings** solve this by generating **different vectors for the same word** depending on its **surrounding context**.\n\n\n\n### 2. **Core Idea**\n\nInstead of assigning a fixed vector to each word, contextual models generate embeddings **dynamically** based on the sentence.\n\nFor example:\n[\n\\text{Embedding(\"bank\", context=\"river\")} \\neq \\text{Embedding(\"bank\", context=\"money\")}\n]\n\nThis is achieved through **deep neural networks** (usually built with LSTMs or Transformers) that process the entire sentence before producing word representations.\n\n\n\n### 3. **Key Models in Contextual Embeddings**\n\nLet’s look at the main models that introduced this idea.\n\n---\n\n### **a. ELMo (Embeddings from Language Models)**\n\n**Developed by:** AllenNLP (2018)\n\n* Based on **bi-directional LSTM language models**.\n* Generates word embeddings that depend on **both left and right context**.\n* Produces different embeddings for the same word in different sentences.\n\n**Architecture:**\n\n* Two-layer bidirectional LSTM trained as a language model.\n* Embeddings are taken from internal states of the network.\n\n**Mathematically:**\n[\n\\text{ELMo}(w_t) = \\gamma \\sum_{l=1}^{L} s_l h_{t,l}\n]\nwhere:\n\n* ( h_{t,l} ) = hidden state of layer ( l ) at position ( t )\n* ( s_l ) = softmax-normalized weights for each layer\n* ( \\gamma ) = scaling factor\n\n**Strength:** Captures **contextual, syntactic, and semantic information**.\n\n\n\n### **b. BERT (Bidirectional Encoder Representations from Transformers)**\n\n**Developed by:** Google AI (2018)\n\n* Based on the **Transformer encoder** architecture.\n* Trained using **Masked Language Modeling (MLM)**:\n\n  * Randomly mask some words and predict them using context from both sides.\n* Produces deeply **contextualized embeddings** for each token.\n\n**Example:**\nInput: “The man went to the **[MASK]**.”\nModel predicts: “store”, “market”, etc., depending on context.\n\n**Advantages:**\n\n* Considers **both left and right context simultaneously**.\n* Learns rich semantic and syntactic information.\n* Easily fine-tuned for downstream tasks (classification, NER, QA, etc.).\n\n\n\n### **c. GPT (Generative Pre-trained Transformer)**\n\n**Developed by:** OpenAI (2018)\n\n* Based on the **Transformer decoder** architecture.\n* Trained as a **causal language model** — predicts the next word given previous words.\n* Generates embeddings based on **left-to-right context** only.\n\n**Key difference from BERT:**\n\n* BERT = bidirectional (understands full context)\n* GPT = unidirectional (good for generation tasks)\n\n\n\n### **d. RoBERTa, ALBERT, and DistilBERT**\n\n* Variants of BERT trained with different optimizations:\n\n  * **RoBERTa:** more data and longer training → better performance\n  * **ALBERT:** fewer parameters (shared layers)\n  * **DistilBERT:** smaller, faster version for edge devices\n\nAll still produce **contextual embeddings** at the token level.\n\n\n\n### 4. **How Contextual Embeddings Work**\n\n1. Input sentence is tokenized.\n2. Model processes the sentence through **multiple Transformer layers**.\n3. Each token’s final hidden state represents its **context-aware embedding**.\n4. These embeddings can then be used for:\n\n   * Classification\n   * Named Entity Recognition (NER)\n   * Translation\n   * Sentiment analysis\n\n\n\n### 5. **Example: Using BERT for Contextual Embeddings**\n\nHere’s a small example using Hugging Face Transformers:\n\n```python\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Input sentence\nsentence = \"He sat by the bank of the river\"\ninputs = tokenizer(sentence, return_tensors='pt')\n\n# Get contextual embeddings\noutputs = model(**inputs)\nembeddings = outputs.last_hidden_state  # Shape: [batch, sequence_length, hidden_dim]\n\nprint(embeddings.shape)\n```\n\nEach token (word or subword) gets a **768-dimensional contextual embedding**.\n\n---\n\n### 6. **Advantages Over Static Embeddings**\n\n| Feature                | Static (Word2Vec/GloVe) | Contextual (BERT/ELMo)      |\n| ---------------------- | ----------------------- | --------------------------- |\n| Representation         | Fixed for each word     | Depends on context          |\n| Handles polysemy       | No                      | Yes                         |\n| Architecture           | Shallow                 | Deep (LSTM/Transformer)     |\n| Language Understanding | Limited                 | Rich semantic understanding |\n| Fine-tuning            | Hard                    | Easy and effective          |\n\n---\n\n### 7. **Applications**\n\n* Sentiment Analysis\n* Question Answering\n* Named Entity Recognition\n* Machine Translation\n* Semantic Search\n* Text Summarization\n\n\n\n### 8. **Key Takeaways**\n\n* Contextual embeddings understand **meaning in context**.\n* ELMo introduced the idea using **BiLSTM**.\n* BERT and GPT improved it using **Transformers**.\n* These embeddings form the **foundation of modern NLP**.","metadata":{}}]}
