{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## `tf.data.Dataset.prefetch()`\n\n### 1. Introduction\nIn TensorFlow’s **`tf.data` API**, the `prefetch()` transformation is used to **overlap the preprocessing and model execution** of data.\n\nNormally, data loading and model training happen **sequentially**:\n1. Load data →  \n2. Preprocess data →  \n3. Feed to model →  \n4. Train →  \n(repeat)\n\nWith `prefetch()`, TensorFlow starts preparing the **next batch of data while the current batch is being processed by the model**, allowing computation and data loading to run **in parallel**.  \n\nThis helps **reduce training bottlenecks** caused by slow data input pipelines.\n\n\n### 2. Function Syntax\n\n```python\ntf.data.Dataset.prefetch(buffer_size)\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Create a simple dataset\ndataset = tf.data.Dataset.range(10)\n\n# Batch and prefetch\ndataset = dataset.batch(2).prefetch(tf.data.AUTOTUNE)\n\nfor batch in dataset:\n    print(batch.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:41:38.121025Z","iopub.execute_input":"2025-10-14T10:41:38.121612Z","iopub.status.idle":"2025-10-14T10:41:38.140684Z","shell.execute_reply.started":"2025-10-14T10:41:38.121584Z","shell.execute_reply":"2025-10-14T10:41:38.139674Z"}},"outputs":[{"name":"stdout","text":"[0 1]\n[2 3]\n[4 5]\n[6 7]\n[8 9]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Without prefetch():\nLoad → Train → Load → Train → Load → Train\n\nWith prefetch():\nLoad + Train overlap → Faster pipeline\n","metadata":{}},{"cell_type":"code","source":"# typical usage in training pipeline\n\ntrain_dataset = (\n    tf.data.Dataset.from_tensor_slices((images, labels))\n    .shuffle(buffer_size=1000)\n    .batch(32)\n    .prefetch(tf.data.AUTOTUNE)\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. Performance Benefits\n\n| Without Prefetch                              | With Prefetch                     |\n| --------------------------------------------- | --------------------------------- |\n| Data loading and training happen sequentially | Data loading and training overlap |\n| GPU often idle waiting for data               | GPU remains busy                  |\n| Slower overall training                       | Faster, smoother training         |\n\nIn large models or heavy input pipelines (e.g., reading images, augmentations), this can significantly **reduce training time**.\n\n---\n\n### 7. Summary\n\n| Concept            | Description                                                                     |\n| ------------------ | ------------------------------------------------------------------------------- |\n| **Purpose**        | To overlap data preprocessing and model execution                               |\n| **How it works**   | Prefetches batches in the background while the current batch is being processed |\n| **Common setting** | `dataset.prefetch(tf.data.AUTOTUNE)`                                            |\n| **Effect**         | Reduces input latency and improves GPU utilization                              |\n| **Use case**       | Always the last step in a `tf.data` pipeline before feeding data into the model |\n\n---\n\n### 8. Best Practice\n\nWhen building any TensorFlow input pipeline:\n\n1. Apply transformations like `map()`, `shuffle()`, and `batch()`.\n2. Always end the pipeline with `.prefetch(tf.data.AUTOTUNE)`.","metadata":{}},{"cell_type":"markdown","source":"## `tf.data.Dataset.cache()`\n\n### 1. Introduction\nIn TensorFlow’s `tf.data` API, the **`cache()`** transformation is used to **store (cache) the dataset in memory or on disk** after the first time it’s loaded.  \n\nThis makes subsequent epochs or iterations **much faster**, since the data doesn’t need to be **re-read** or **re-preprocessed** each time.\n\nIt’s especially useful when:\n- You’re repeatedly iterating over the same dataset (e.g., during multiple training epochs).\n- The dataset fits into memory (RAM) or can be cached to disk efficiently.\n\n\n### 2. Function Syntax\n\n```python\ntf.data.Dataset.cache(filename='')\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport time\n\n# Create a simple dataset\ndataset = tf.data.Dataset.range(5)\n\n# Simulate a slow preprocessing step\ndataset = dataset.map(lambda x: tf.py_function(lambda y: time.sleep(0.5) or y, [x], tf.int64))\n\n# Cache the dataset\ndataset = dataset.cache()\n\n# Iterate twice (simulating 2 epochs)\nfor epoch in range(2):\n    start = time.time()\n    for item in dataset:\n        print(f\"Epoch {epoch+1}: Item {item.numpy()}\")\n    print(\"Time:\", round(time.time() - start, 2), \"seconds\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:44:41.926485Z","iopub.execute_input":"2025-10-14T10:44:41.927223Z","iopub.status.idle":"2025-10-14T10:44:44.519498Z","shell.execute_reply.started":"2025-10-14T10:44:41.927193Z","shell.execute_reply":"2025-10-14T10:44:44.518660Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Item 0\nEpoch 1: Item 1\nEpoch 1: Item 2\nEpoch 1: Item 3\nEpoch 1: Item 4\nTime: 2.53 seconds\n\nEpoch 2: Item 0\nEpoch 2: Item 1\nEpoch 2: Item 2\nEpoch 2: Item 3\nEpoch 2: Item 4\nTime: 0.0 seconds\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Use Case: Caching in Training Pipelines\n\n# A common usage pattern:\n\ntrain_dataset = (\n    tf.data.Dataset.from_tensor_slices((images, labels))\n    .map(preprocess_function)\n    .cache()  # Cache after preprocessing\n    .shuffle(1000)\n    .batch(32)\n    .prefetch(tf.data.AUTOTUNE)\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}