## 1. Introduction to makemore and Language Modeling

The video introduces **makemore**, a character-level language model designed to generate new data that resembles the training data.

| Concept | Explanation & Example |
| :--- | :--- |
| **makemore** | A tool built to *make more* examples of a given dataset, in this case, a list of **32,000 unique names** from `names.txt` [[00:23](http://www.youtube.com/watch?v=PaCmpygFfXo&t=23)]. |
| **Character-Level Language Model** | This model operates at the level of individual characters. Its task is to predict the **probability of the next character** given the preceding character(s) [[01:28](http://www.youtube.com/watch?v=PaCmpygFfXo&t=88)].|
| **Bigram Model** | The simplest form of this model, which uses **only the immediate previous character** to predict the next one. For a word like "Anna," the model predicts 'n' given 'A', then 'n' given 'n', and 'a' given 'n'. |
| **Dataset Padding** | To model the start and end of a name, a special **start token** (`<S>` or `.`) and an **end token** (`<E>` or `.`) are added to each word. This allows the model to learn the probability of a word *beginning* with a character and *ending* with one [[08:28](http://www.youtube.com/watch?v=PaCmpygFfXo&t=508)]-[[09:02](http://www.youtube.com/watch?v=PaCmpygFfXo&t=542)]. |
| **Example (Bigrams for 'emma')** | The word `emma` is treated as a sequence of bigrams: `<S>e`, `em`, `mm`, `ma`, `a<E>`. |

***

## 2. Statistical Training: The Count-Based Approach

The initial, non-neural network approach to building the bigram model relies entirely on counting character sequences in the training data.

| Concept | Explanation & Example |
| :--- | :--- |
| **Counts Matrix (N)** | A **27x27 matrix** (tensor) is created to store the raw frequencies of every possible bigram. The 27 characters include the 26 lowercase English letters plus the special start/end token [[11:01](http://www.youtube.com/watch?v=PaCmpygFfXo&t=661)]. |
| **Indexing** | Characters must be mapped to integers (e.g., `.` maps to 0, `a` maps to 1, `b` maps to 2, etc.) to use them as array indices. This allows for efficient storage and lookup [[01:15:42](http://www.youtube.com/watch?v=PaCmpygFfXo&t=4542)]-[[01:16:42](http://www.youtube.com/watch?v=PaCmpygFfXo&t=4602)]. |
| **Probability Distribution (P)** | The raw counts in the `N` matrix are converted into probabilities by **normalizing each row**. This means dividing every count in a row by the sum of that row. Each row of the resulting matrix `P` is a valid probability distribution (sums to 1) for the next character, *given the current character* [[01:56:34](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6994)]. |
| **Model Generation (Sampling)** | New names are generated by repeatedly **sampling** from the model's probability distribution `P`. You start with the start token, use its row in `P` to randomly pick the next character, and repeat this process until the end token is sampled [[01:54:34](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6874)]. |

***

## 3. Model Evaluation and Optimization

To assess the quality of the model's predictions, a loss function is introduced.

| Concept | Explanation & Example |
| :--- | :--- |
| **Negative Log Likelihood (NLL) Loss** | This is the primary metric used to evaluate the model. The goal is to **minimize** this loss [[01:56:24](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6984)]. A low NLL means the model is confidently assigning a high probability to the *actual* next character observed in the training data. |
| **NLL for a single bigram** | $\text{Loss} = - \log(\text{P}(\text{Next Char} \mid \text{Current Char}))$ |
| **Smoothing / Regularization** | To prevent division by zero or the negative log of zero (which results in infinite loss), a regularization technique is used. By adding a **small uniform count** (e.g., 1) to every entry in the raw counts matrix $N$, the model avoids assigning a zero probability to any bigram, ensuring the loss remains finite and realistic [[01:53:47](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6827)]. |

***

## 4. Neural Network Training: The Gradient-Based Approach

The video demonstrates how to transition the bigram model into a simple neural network that yields the *identical* result as the count-based approach, but is flexible enough to scale to more complex models.

| Concept | Explanation & Example |
| :--- | :--- |
| **One-Hot Encoding** | The current character (e.g., 'a') is encoded as a vector of all zeros, except for a '1' in the position corresponding to its integer index (e.g., `[0, 1, 0, ..., 0]` for 'a'). This is the input to the neural network [[01:55:10](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6910)]. |
| **Linear Layer (Weights $W$)** | The count matrix $N$ is replaced by a **Weight Matrix ($W$)**. The input vector (one-hot) is multiplied by $W$. This multiplication effectively *plucks out* the row of $W$ corresponding to the current character, just like looking up the row in the count matrix [[01:55:18](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6918)]-[[01:55:26](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6926)].|
| **Logits and Softmax** | The output of the linear layer is called the **logits** (raw prediction scores). These logits are analogous to the *log of the raw counts*. To convert them into a probability distribution, the **softmax** function is applied, which involves exponentiating the logits and then normalizing the results [[01:55:33](http://www.youtube.com/watch?v=PaCmpygFfXo&t=6933)]. |
| **Gradient Descent** | Instead of calculating counts directly, the model starts with random weights ($W$). The NLL loss is calculated, and the **gradient** (the direction to adjust the weights) is computed to iteratively update $W$ to minimize the loss. This gradient-based optimization converges to the same bigram probabilities found by the simple count method [[01:56:50](http://www.youtube.com/watch?v=PaCmpygFfXo&t=7010)].|

The neural network framework, though initially equivalent to the count-based approach, is highlighted as the foundation for building more complex models like transformers, which are introduced as the next step in the series [[01:57:34](http://www.youtube.com/watch?v=PaCmpygFfXo&t=7054)].

You can watch the video here: [The spelled-out intro to language modeling: building makemore](http://www.youtube.com/watch?v=PaCmpygFfXo)
